\chapter{Conclusions}
\label{chapterConclusions}


This chapter concludes the thesis.  Firstly, we summarise the main
results of this thesis.  For the remainder of this chapter, we discuss
insights gained from this work, and possible directions for future
research.

\section{Summary of the thesis}

We set out to test and verify natural language grammars. In this
thesis, we have described two systems for two different grammar
formalisms: GF and CG.

When testing GF grammars, earlier we relied on an informant to
remember and explain all important phenomena to test, and a
grammarian to form all relevant trees that showcase these phenomena. 
% Now, we have a system that takes a GF grammar, a concrete language and a
% list of functions and/or categories. Then the system generates a
% representative and minimal set of example sentences that showcases all
% important variation in the given functions and categories.
Now both steps are fully automated: the grammarian only needs to input
a function or category, and the system outputs a minimal and
representative set of sentences that showcases all the variation in
the given functions and categories. The informant only needs to read and
evaluate the example sentences.
Of course, humans are still needed to write the grammars in the
first place, but we consider that a fun task.

For CG, previous evaluation methods relied on a gold standard corpus,
or manual inspection if there was no corpus available. Our method
tests the internal consistency of a grammar, only relying on a
morphological lexicon. The system explores all possibilities to see if
a particular rule can apply: if there is no input that would trigger a
rule $r$ after rule $s$, we know that rules $r$ and $s$ are in
conflicting.  In addition, the mechanisms that we developed in order
to test CG have interesting side effects: namely, they let us model CG
as a generative formalism.

% Our method only tests the internal consistency of a grammar. It is a
% separate question whether the CG grammar is also good for
% disambiguating natural language. Such questions are better left for
% corpus-based methods.


% If we were sculpting a statue, generative formalism would start from an empty place and start splashing some clay around. In contrast, a reductionist formalism would start from a heavy stone block and carve away everything that is not part of the statue. In the end, both have formed an identical statue. An empty generative grammar would output just thin air, and empty reductionist grammar would output an intact stone block.

% We can see symbolic sentences as these stone blocks. To give a classic example, say we have the alphabet {a,b} and the language aⁿbⁿ. Then we can construct a CG grammar, apply it to an even-length symbolic sentence consisting of as and bs, and have it output a string where the first half is as, followed by a second half of bs. This language is context-free, and we have only found a method to write such a grammar manually, but we have a method that can translate any regular language into a corresponding CG automatically, and apply it to a symbolic sentence.


\section{Insights and future directions}


\subsection{Completeness}

Let us start from CG, because it is easier to reason about. A CG
grammar can be “good” or “bad” according to purely internal
criteria. Good means just that the rules are internally consistent,
and bad the opposite of that. The point of symbolic evaluation is to
explore all possibilities: if there was a way in which some rule would
apply (e.g. make another rule not apply and thus preserving a reading;
or create a solution in which a particular reading was already false),
the SAT-solver would have found it. So we can be confident that if the
system reports no conflicts, there are no conflicts---``what if we
just run a few more tests'' doesn’t make sense to worry about.

It is a separate question (not my research question!) whether the CG
grammar also does a good job at disambiguating natural language. Such
questions are better left for corpus-based methods.

On the GF side, we actually are trying to answer the harder question:
does this grammar do an adequate job at producing
English/French/\dots? For that, we need a set of test sentences that
covers all phenomena in the grammar. In the CG world, we had a natural
restriction: there is only a few hundreds or thousands of different
tags in the morphological lexicon, and there are restrictions in how
they mix together into readings. So the language of the tags and
readings is finite—sounds reasonable that we can do the job
thoroughly.

But natural language is infinite. How does that work with a finite set
of test cases? First of all, we are testing a \pmcfg{} representation
of natural language. This representation may still define an infinite
language: for example, the grammar can parse and produce a sentence
with arbitrarily many relative clauses (\emph{I know that you think
  that she disagrees that \dots this example is contrived}), only
limited by physical constraints such as memory or having a source of
power after all humans die. But importantly, the grammar still has a
finite number of \emph{functions}, and all those functions can only
take arguments of finitely many different types.

How about repeating the same function arbitrarily many times? Could it
be that the tree for {\em you think that X} is linearised correctly,
but {\em I know that you think that X} introduces an error?  Luckily,
we can answer this definitely.  In the \pmcfg{} format, all variation
is encoded as parameters: if the $n^{th}$ repetition of some function
would cause different behaviour, then there would be a parameter that
counts the applications, and the program would generate separate test
cases for one relative clause, and $n$ nested relative clauses.


% “How can we test a whole big infinite grammar?”

% Easy: there’s a finite amount of functions, just test each function separately.

% “How can we be sure to use one function in all the different ways?”

% No problem: split it into several functions that can all be used in
% only one way, then test each of them.

\subsection{Failure modes}

A given CG grammar is internally consistent given a certain tagset,
rules to combine tags into readings, and ambiguity classes. % All of
% this information comes from morphological lexica, which are not
% guaranteed to be infallible.
In other words, CG analysis is as reliable as is the morphological
lexicon where the tags and the readings come from. Quite simply, if
the morphological lexicon contains a word with the wrong tag, or the
specifications for how tags can combine into readings are outdated,
our tool may do the wrong thing:  or fail to
detect a real error.

How about GF test cases? We know that the system produces a test case
for each combination of parameters---and the failure mode here is,
simply, not enough parameters.  Consider the \t{AdjCN} function for
Spanish again, and say that we don’t have the parameter for pre- and
postmodifier at all, and \t{AdjCN} only comes in two variants, one for
feminine and one for masculine. Suppose that all adjectives are,
incorrectly, postmodifiers. Then we would notice the error only if the
single \t{AP} argument for \t{AdjCN$_{\text{fem}}$} and
\t{AdjCN$_{\text{masc}}$} happened to be a premodifier one.

If the missing parameter handles a common phenomenon, it is more
likely that we catch the error even with missing parameters. This was
the case with, for example, Dutch preposition contraction (from
Section~\ref{gf-testing-examples}). Originally, none of the
prepositions contracted, but this is the most common behaviour---only a
handful of prepositions don’t contract. Thus we got some test cases
which showed the error, and went on to fix the grammar. After the
fixes, the system created separate test cases for contracting and
non-contracting prepositions.

\subsection{Future directions}

After two years of the initial publication
\cite{listenmaa_claessen2016}, the CG analysis tool has, to our
knowledge, never been used outside our research group. A natural
explanation is that our research group has never developed CG
grammars, and we did not make an effort to write a user-friendly tool
that is easy to adopt into an existing workflow.  We still believe
that the CG analysis tool has its merits, and even if our
implementation doesn't catch on, we leave a description in this
document.

In contrast, This time we had a better recipe for success: we were
much more invested in making the tool user-friendly, integrating it to
common workflows, and most importantly, using it ourselves.  The test
suite generation tool for GF grammars is being adopted by the GF
community: at the time of writing this page, we have 2 users outside
the research group, and based on personal anecdotes, it is proving to
be useful for grammar writing.

\todo{Some actual future directions}