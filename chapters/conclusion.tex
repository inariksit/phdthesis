\chapter{Conclusions}
\label{chapterConclusions}


This chapter concludes the thesis.  Firstly, we summarise the main
results of this thesis.  For the remainder of this chapter, we discuss
insights gained from this work, and possible directions for future
research.

\section{Summary of the thesis}

We set out to test and verify natural language grammars. In this
thesis, we have described two systems for two different grammar
formalisms: Grammatical Framework (GF) and Constraint Grammar (CG).

When testing GF grammars, earlier we relied on an informant to
remember and explain all important phenomena to test, and a
grammarian to form all relevant trees that showcase these phenomena. 
% Now, we have a system that takes a GF grammar, a concrete language and a
% list of functions and/or categories. Then the system generates a
% representative and minimal set of example sentences that showcases all
% important variation in the given functions and categories.
Now both steps are fully automated: the grammarian only needs to input
a function or category, and the system outputs a minimal and
representative set of sentences that showcases all the variation in
the given functions and categories. The informant only needs to read and
evaluate the example sentences.
(Of course, humans are still needed to write the grammars in the
first place, but we consider that a fun task.)

For CG, previous evaluation methods relied on a gold standard corpus,
or manual inspection if there was no corpus available. Our method
tests the internal consistency of a grammar, only relying on a
morphological lexicon. The system explores all possibilities to see if
a particular rule can apply: if there is no input that would trigger a
rule $r$ after rule $s$, we know that rules $r$ and $s$ are in
conflict.  In addition, the mechanisms that we developed in order
to test CG have interesting side effects: namely, they let us model CG
as a generative formalism.

\todo{Can put more stuff here---even copy stuff from elsewhere in thesis}

% If we were sculpting a statue, generative formalism would start from an empty place and start splashing some clay around. In contrast, a reductionist formalism would start from a heavy stone block and carve away everything that is not part of the statue. In the end, both have formed an identical statue. An empty generative grammar would output just thin air, and empty reductionist grammar would output an intact stone block.

% We can see symbolic sentences as these stone blocks. To give a classic example, say we have the alphabet {a,b} and the language aⁿbⁿ. Then we can construct a CG grammar, apply it to an even-length symbolic sentence consisting of as and bs, and have it output a string where the first half is as, followed by a second half of bs. This language is context-free, and we have only found a method to write such a grammar manually, but we have a method that can translate any regular language into a corresponding CG automatically, and apply it to a symbolic sentence.


\section{Insights and future directions}

\subsection{On completeness}

Testing can only show the presence of bugs, not the absence. The tools
introduced in this thesis are no exception: here we comment on the
reliability and completeness of both systems.


\paragraph{CG}

The quality of a CG grammar can be judged according to purely internal
criteria: either the rules are internally consistent or not. It is a
separate question (not the research question in this thesis!) whether
the CG grammar also does a good job at disambiguating natural
language. Such questions are better left for corpus-based methods.

To be more precise, a CG grammar is internally consistent given a
certain tagset, rules to combine tags into readings, and ambiguity
classes. % All of
% this information comes from morphological lexica, which are not
% guaranteed to be infallible.
This means that our CG analysis is as reliable as is the morphological
lexicon where the tags and the readings come from. Quite simply, if
the morphological lexicon contains a word with the wrong tag, or the
specifications for how tags can combine into readings are outdated,
our tool may do the wrong thing: report a false conflict or fail to
detect a real error.

But if the lexicon and tag set are correct, then we can be fairly
certain of the analysis. If the grammar is deemed conflict-free, it
means that for each rule $r$ in the grammar, the program has found a
sequence of words which can pass through all previous rules, and $r$
can remove something from at least one word. In other words, the
system has to \emph{generate} a positive example for all rules $r$ it
judges conflict-free. %with all of the previous rules.
This $r$ doesn't even need to do the right thing---it may as well be
``\emph{wish} is a verb in the context \emph{the wish}''---but we can
still count on it being conflict-free.

% Contrast this with ``we tested 1+3 and got 4, thus we know that
% addition is implemented correctly''.

Can we trust the program when it finds $r$ being in conflict with some
other rule? We use symbolic evaluation to explore all possibilities:
if there was a way in which $r$ could apply (e.g. create a context
which prevents a previous rule from matching), the SAT-solver would
have found it. If we find $r$ impossible to apply, it has to be one of
the following causes: (a) its conditions are inconsistent (e.g. ``-1
has to be a noun and not be a noun'')\footnote{If we have ambiguity
  classes in place, the inconsistency can also be e.g. ``-1 has to be
  a noun and a verb'', assuming that there is no word form in the lexicon that
  is ambiguous between noun and verb.}; (b) another rule or set of
rules before it forces the symbolic sentence to be in a certain way.


\todo{Something to finish up this}



\paragraph{GF} On the GF side, we actually are trying to answer the
harder question: does the grammar do an adequate job at producing
some natural language? For that, we need a set of test sentences that
cover all phenomena in the grammar. In the CG world, we had a natural
restriction: there is only a few hundreds or thousands of different
tags in the morphological lexicon, and there are restrictions in how
they mix together into readings. So the language of the tags and
readings is finite---sounds reasonable that we can do the job
thoroughly.

But natural language is infinite. How does that work with a finite set
of test cases? First of all, we are testing a \pmcfg{} representation
of natural language. This representation may still define an infinite
language: for example, the grammar can parse and produce a sentence
with arbitrarily many relative clauses (\emph{I know that you think
  that she disagrees that \dots this example is contrived}), only
limited by physical constraints such as memory or having a source of
power after all humans die. But importantly, the grammar still has a
finite number of \emph{functions}, and all those functions can only
take arguments of finitely many different types.

How about repeating the same function arbitrarily many times? Could it
be that the tree for {\em you think that X} is linearised correctly,
but {\em I know that you think that X} introduces an error?  Luckily,
we can answer this definitely.  In the \pmcfg{} format, all variation
is encoded as parameters: if the $n^{th}$ repetition of some function
would cause different behaviour, then there would be a parameter that
counts the applications, and the program would generate separate test
cases for one relative clause, and $n$ nested relative clauses.

But we can still fail to generate a test that reveals the error---the
failure mode is, simply, not enough parameters.  Consider the
\t{AdjCN} function for Spanish again, and say that we don’t have the
parameter for pre- and postmodifier at all, and \t{AdjCN} only comes
in two variants, one for feminine and one for masculine. Suppose that
all adjectives are, incorrectly, postmodifiers. Then we would notice
the error only if the single \t{AP} argument for \t{AdjCN$_{\text{fem}}$} 
and \t{AdjCN$_{\text{masc}}$} happened to be a premodifier one.

If the missing parameter handles a common phenomenon, it is more
likely that we catch the error even with missing parameters. This was
the case with, for example, Dutch preposition contraction (from
Section~\ref{gf-testing-examples}). Originally, none of the
prepositions contracted, but this is the most common behaviour---only a
handful of prepositions don’t contract. Thus we got some test cases
which showed the error, and went on to fix the grammar. After the
fixes, the system created separate test cases for contracting and
non-contracting prepositions.

\subsection{Future directions}

After two years of the initial publication
\cite{listenmaa_claessen2016}, the CG analysis tool has, to our
knowledge, never been used outside our research group. A natural
explanation is that our research group has never developed CG
grammars, thus we have less clear idea about existing workflows and
the concrete needs of CG grammarians, and less chances to attract
users. % Even if we had gotten users at the beginning, our chances of
% providing long-term maintenance would have been quite low.
We still believe that the CG analysis tool has its merits, and
even if our implementation doesn't catch on, we leave the idea
documented in this thesis.


% Writing a proof-of-concept program may not be simple, but at least the
% work is over once the program is finished. In contrast, creating a
% usable tool requires a lot of work after the initial implementation.
% It would make more sense that this work is done by people already in
% the community. 
% There is a working implementation which anyone interested can
% try, but at this point it seems that it is the idea, rather than the 
% implementation, that we should pitch.


With the tool for generating GF test suites
\cite{listenmaa_claessen2018}, we had a better recipe for success: we
were much more invested in making the tool user-friendly, integrating
it to common workflows, and most importantly, using it ourselves. The
tool is being adopted by the GF community: at the time of writing this
page, we have 3 known users outside the research group, and based on
personal anecdotes, it is proving to be useful for grammar writing. We
are also much more committed in long-term maintenance of the tool.

There are many directions to develop the GF test suite tool further.
As we mention in Section~\ref{gf-future}, there is no semantic
coherence in the generated sentences---this is particularly important
when working with non-linguist testers.  Another interesting feature
is shrinking the test sentences. Especially for resource grammars, the
generated examples are quite long: this is due to the preference for
maximally unique fields, and ``always run with Mary'' is a more
distinguishing example of a verb phrase than ``run''. However, if an
error is found in a sentence with ``always run with Mary'', it would
be desirable to generate another example with smaller examples, such
as ``run'', ``always run'' and ``run with Mary'', to help narrow down
which function introduces the error.

In more philosophical terms, we would like the grammar writing
community to view testing, in terms of \citet{beizer2003software}, as a
``mental discipline that helps all IT professionals develop
higher-quality software.'' \todo{this or something else more general
  thing to finish this thing.}

% What are some factors that would help a development organization move
% from Beizer’s testing level 2 (testing is to show errors) to testing level 4
% (a mental discipline that increases quality)?