\chapter{Conclusions}
\label{chapterConclusions}


This chapter concludes the thesis.  Firstly, we summarise the main
results of this thesis.  For the remainder of this chapter, we discuss
insights gained from this work, and possible directions for future
research.

\section{Summary of the thesis}

We set out to test and verify natural language grammars. In this
thesis, we have described two systems for two different grammar
formalisms: Constraint Grammar (CG) and Grammatical Framework (GF).

For CG, previous evaluation methods relied on a gold standard corpus,
or manual inspection if there was no corpus available. Our work
started with the observation that CG rules resemble logical formulas;
{\sc remove} and {\sc if} can be expressed as negations and
implications respectively. As noted by \citet{lager98}, a
straight-forward implementation in a logical framework results in a
parallel and unordered CG.

Our implementation of the sequential CG is very similar to the
encoding by \citet{lager_nivre01}.  However, we modified the setup in
a crucial detail: instead of an actual sentence, we applied the rules
to a {\em symbolic sentence}, where every word contains every possible
reading.  Then, instead of the original question ``Which readings are
true after all rule applications?'', we found a meaningful
SAT-problem, with the question ``Which readings were originally
true?''

We exploited this newly found property for grammar analysis.
%  for each
% rule $r$, we tried to create a sentence which has passed through all
% previous rules $R$, and could still trigger $r$.  If such sentence
% could not be found, this means that some of the rules in $R$ made $r$
% impossible to apply: for instance, by removing the same target with
% less conditions, or removing something from the context of $r$.  This
% method was successful in finding such conflicts, when tested with
% grammars between 60--1200 rules.
Our method tests the internal consistency of a grammar, only relying
on a morphological lexicon. The system explores all possibilities to
see if a particular rule can apply: if there is no input that would
trigger a rule $r$ after rule $r'$, we know that rules $r$ and $r'$
are in conflict. This method was successful in finding such conflicts,
when tested with grammars between 60--1200 rules.  In addition, the
mechanisms that we developed in order to test CG have interesting side
effects: namely, they let us model CG as a generative formalism, which
opens potential for deriving CG grammars from other grammar formalisms.

When testing GF grammars, earlier we relied on an informant to
remember and explain all important phenomena to test, and a grammarian
to form all relevant trees that showcase these phenomena.  This time,
our inspiration was all the work done for automatic test case
generation for general software, such as
\citet{celentano1980compiler,Geist1996,QuickCheck}. Software testing
deals with notions of coverage and compactness, and deriving test
cases from a formal specification; this proved to be a useful approach
for grammar testing as well.

% Now, we have a system that takes a GF grammar, a concrete language and a
% list of functions and/or categories. Then the system generates a
% representative and minimal set of example sentences that showcases all
% important variation in the given functions and categories.
In this thesis, we have presented a tool that automates the two steps:
identifying the grammatical phenomena to test and formulating them as
trees. Now, the grammarian only needs to input a function or category,
and the system outputs a minimal and representative set of sentences
that showcases all the variation in the given function or
category. The informant only needs to read and evaluate the example
sentences.  (Of course, humans are still needed to write the grammars
in the first place, but we consider that a fun task!)

%
% If we were sculpting a statue, generative formalism would start from an empty place and start splashing some clay around. In contrast, a reductionist formalism would start from a heavy stone block and carve away everything that is not part of the statue. In the end, both have formed an identical statue. An empty generative grammar would output just thin air, and empty reductionist grammar would output an intact stone block.

% We can see symbolic sentences as these stone blocks. To give a classic example, say we have the alphabet {a,b} and the language aⁿbⁿ. Then we can construct a CG grammar, apply it to an even-length symbolic sentence consisting of as and bs, and have it output a string where the first half is as, followed by a second half of bs. This language is context-free, and we have only found a method to write such a grammar manually, but we have a method that can translate any regular language into a corresponding CG automatically, and apply it to a symbolic sentence.


\section{Insights and future directions}

\subsection{On completeness}

Testing can only show the presence of bugs, not the absence. The tools
introduced in this thesis are no exception: here we comment on the
reliability and completeness of both systems.


\paragraph{CG}

The quality of a CG grammar can be judged according to purely internal
criteria: either the rules are internally consistent or not. It is a
separate question (not the research question in this thesis!) whether
the CG grammar also does a good job at disambiguating natural
language. Such questions are better left for corpus-based methods.

To be more precise, a CG grammar is internally consistent given a
certain tagset, rules to combine tags into readings, and ambiguity
classes. % All of
% this information comes from morphological lexica, which are not
% guaranteed to be infallible.
This means that reliability of our CG analysis tool depends on the
consistency of the CG grammar and the morphological lexicon where the
tags and the readings come from. Quite simply, if the morphological
lexicon contains a word with the wrong tag, or the specifications for
how tags can combine into readings are outdated, our tool may do the
wrong thing: report a false conflict or fail to detect a real error.

But if the lexicon and tag set are correct, then we can be fairly
certain of the analysis. If the grammar is deemed conflict-free, it
means that for each rule $r$ in the grammar, the program has found a
sequence of words which can pass through all previous rules, and $r$
can remove something from at least one word. In other words, the
system has to \emph{generate} a positive example for all rules $r$ it
judges conflict-free. %with all of the previous rules.
This $r$ doesn't even need to do the right thing---it may as well be
``\emph{wish} is a verb in the context \emph{the wish}''---but we can
still trust that $r$ doesn't conflict with any of the previous rules
in the grammar.



Can we trust the program when it finds $r$ being in conflict with some
other rule? We use symbolic evaluation to explore all possibilities:
if there was a way in which $r$ could apply (e.g. create a context
which prevents a previous rule from matching), the SAT-solver would
have found it. If we find $r$ impossible to apply, it has to be one of
the following causes: (a) its conditions are inconsistent (e.g. ``-1
has to be a noun and not be a noun'')\footnote{If we have ambiguity
  classes in place, the inconsistency can also be more subtle,
  e.g. ``-1 has to be noun and punctuation'', assuming that there is
  no word form in the lexicon that has such an ambiguity.}, or (b)
another rule $r'$ or set of rules $R'$ before it forces the symbolic
sentence to be in a certain way. In the case of (a), the human
grammarian would hopefully see the error when it is pointed out. In
the case of (b), even if the conflict is not clear by just looking at
the grammar, the SAT-solver can help by generating a positive example,
i.e. a sequence that triggers $r$ when it goes through all the
previous rules except $r'$.

% Contrast this with ``we tested 1+3 and got 4, thus we know that
% addition is implemented correctly''.

\paragraph{GF} On the GF side, we actually are trying to answer the
harder question: does the grammar do an adequate job at producing
some natural language? For that, we need a set of test sentences that
cover all phenomena in the grammar. In the CG world, we had a natural
restriction: there is only a few hundreds or thousands of different
tags in the morphological lexicon, and there are restrictions in how
they mix together into readings. So the language of the tags and
readings is finite---sounds reasonable that we can do the job
thoroughly.

But natural language is infinite. How does that work with a finite set
of test cases? First of all, we are testing a \pmcfg{} representation
of natural language. This representation may still define an infinite
language: for example, the grammar can parse and produce a sentence
with arbitrarily many subordinate clauses (\emph{I know that you think
  that she disagrees that \dots this example is contrived}), only
limited by physical constraints such as memory or having a source of
power after all humans die. But importantly, the grammar still has a
finite number of \emph{functions}, and all those functions can only
take arguments of finitely many different types.

How about repeating the same function arbitrarily many times? Could it
be that the tree for {\em you think that X} is linearised correctly,
but {\em I know that you think that X} introduces an error?  Luckily,
we can answer this definitely.  In the \pmcfg{} format, all variation
is encoded as parameters: if the $n^{th}$ repetition of some function
would cause different behaviour, then there would be a parameter that
counts the applications, and the program would generate separate test
cases for one subordinate clause, and $n$ nested subordinate clauses.

But we can still fail to generate a test that reveals the error---the
failure mode is, simply, not enough parameters.  Consider the
\t{AdjCN} function for Spanish again, and say that we don’t have the
parameter for pre- and postmodifier at all, and \t{AdjCN} only comes
in two variants, one for feminine and one for masculine. Suppose that
all adjectives are, incorrectly, postmodifiers. Then we would notice
the error only if the single \t{AP} argument for \t{AdjCN$_{\text{fem}}$} 
and \t{AdjCN$_{\text{masc}}$} happened to be a premodifier one.

If the missing parameter handles a common phenomenon, it is more
likely that we catch the error even with missing parameters. This was
the case with, for example, Dutch preposition contraction (from
Section~\ref{gf-testing-examples}). Originally, none of the
prepositions contracted, but this is the most common behaviour---only a
handful of prepositions don’t contract. Thus we got some test cases
which showed the error, and went on to fix the grammar. After the
fixes, the system created separate test cases for contracting and
non-contracting prepositions.

\subsection{Future directions}

% After two years of the initial publication
% \cite{listenmaa_claessen2016}, the CG analysis tool has, to our
% knowledge, never been used outside our research group. % A natural
% explanation is that our research group has never developed CG
% grammars, thus we have less clear idea about existing workflows and
% the concrete needs of CG grammarians, and less chances to attract
% users. % Even if we had gotten users at the beginning, our chances of
% % providing long-term maintenance would have been quite low.
% We still believe that the CG analysis tool has its merits, and
% even if our implementation doesn't catch on, we leave the idea
% documented in this thesis.
% Writing a proof-of-concept program may not be simple, but at least the
% work is over once the program is finished. In contrast, creating a
% usable tool requires a lot of work after the initial implementation.
% It would make more sense that this work is done by people already in
% the community. 
% There is a working implementation which anyone interested can
% try, but at this point it seems that it is the idea, rather than the 
% implementation, that we should pitch.
% However, the tool for generating GF test suites
% \cite{listenmaa_claessen2018}, we had a better recipe for success: we
% were much more invested in making the tool user-friendly, integrating
% it to common workflows, and most importantly, using it ourselves. The
% tool is being adopted by the GF community: at the time of writing this
% page, we have 3 known users outside the research group, and based on
% personal anecdotes, it is proving to be useful for grammar writing. We
% are also much more committed in long-term maintenance of the tool.

The tools are available, and have had already some usage outside the
research group. Out of the two tools, the GF test suite generation
shows more potential for wider adoption and becoming a standard
practice of GF grammar engineering.

There are many directions to develop the GF test suite tool further.
As we mention in Section~\ref{gf-future}, there is no semantic
coherence in the generated sentences---this is particularly important
when working with non-linguist testers. However, if it turns out that
most testing work is done on application grammars, this step may not
be as crucial

Another interesting feature is shrinking the test
sentences. Especially for resource grammars, the generated examples
are quite long: this is due to the preference for maximally unique
fields, and ``always run with Mary'' is a more distinguishing example
of a verb phrase than ``run''. However, if an error is found in a
sentence with ``always run with Mary'', it would be desirable to
generate another example with smaller examples, such as ``run'',
``always run'' and ``run with Mary'', to help narrow down which
function introduces the error.

\todo{Workflow: language workbench}

In more philosophical terms, we would like the grammar writing
community to view testing, in terms of \citet{beizer2003software}, as
a ``mental discipline that helps all IT professionals develop
higher-quality software.''  Our work is by no means the first approach
to grammar testing: for instance, \citet{butt1999lfg} recommend
frequent use of test suites, as well as extensive documentation of
grammars. We argue that generating test suites automatically from the
grammar itself, combined with treebanks for coverage testing, is an
even better way to test grammars. It is always a risk that a test
suite is deprecated when the grammar is changed; with automatic
generation, the test suite is updated along with the grammar.




% What are some factors that would help a development organization move
% from Beizer’s testing level 2 (testing is to show errors) to testing level 4
% (a mental discipline that increases quality)?