\chapter{Introduction to this thesis}

There are many approaches to natural language processing (NLP). 
\todo{More setup, explanation what are data-driven and rule-based, make forward reference to Chapter 2}
\todo{Put something like this already here: ``Our work is by no means the first approach
to grammar testing: for instance, \citet{butt1999lfg} recommend
frequent use of test suites, as well as extensive documentation of
grammars.''}

%Nowadays, many well-known end-user applications are based on machine learning.
A data-driven approach is appropriate, when the target language is well-resourced (and grammatically simple), the domain is unlimited and correctness is not crucial.
For example, a monolingual English speaker who wants to get the gist of any non-English web page will be happy with a free, wide-coverage online service, even if it makes some mistakes.

Rule-based NLP has a different set of use cases. Writing grammar rules takes more human effort than training a machine learning model, but in many cases, it is the more feasible approach:
%but the results are more general, predictable and adaptable. 



\begin{itemize}
\setlength\itemsep{0.0em}
\item \textbf{Quality over coverage.} 
Think of producers instead of consumers of information: the producer needs high guarantees of correctness,
but often there is only a limited domain to cover. %Grammars are ideal for such use cases.
%In such a case, a domain-specific grammar %for translation or natural language generation
%is much more feasible than a general-purpose tool, with wide coverage but limited quality.
%A user 
%In contrast, producers of information need strong guarantees that the information they provide is correct in all languages. 

\item \textbf{Less language resources needed}. Most of the 6000+ languages of the world do not have the abundance of data needed for machine learning, making rule-based approach the only feasible one.

\item \textbf{Grammatically complex languages} benefit proportionately more from a grammar-based approach. 
Think of a grammar as a compression method: a compact set of rules generates infinitely many sentences. 
Grammars can also be used in conjunction with machine learning, e.g. creating new training data to prevent data sparsity.

\item \textbf{Grammars are explainable}, and hence, testable. If there is a bug in a particular sentence, we can find the reason for it and fix it. In contrast, machine learning models are much more opaque, and the user can just tweak some parameters, without guarantees how it affects the model.

\end{itemize}

%Rule-based and data-driven NLP have different trade-offs: the first prioritizes quality over coverage, and vice versa. It is especially applicable for producers of information, as opposed to consumers: a user who just wants to get the gist of a web page in a foreign language benefits from a wide-coverage system, even if it makes mistakes. In contrast, producers of information need strong guarantees that the information they provide is correct in all languages.

%Another area is lesser-resourced languages. Data-driven methods require large amounts of data, and while major languages such as English, Spanish and Chinese have an abundance of data, the majority of the 6000 \cite{citesomething} languages of the world are in a less fortunate.

%In addition, more complex languages benefit proportionately more from rule-based approach. If a single noun or verb can have hundreds of inflection forms (as opposed to English, think \emph{dog} and \emph{dogs}), we would need much more training data in order to cover all of them. A grammar-based approach can be used to synthesize data, in conjunction with a data-driven approach such as deep learning.
%This benefits even languages that are relatively well-off, such as Finnish.

Testing grammars has one big difference from testing software: natural language has no formal specification, 
so ultimately we must involve a human oracle. However, we can automate many useful subtasks: detect \emph{ambiguous constructions} and \emph{contradictory grammar rules}, as well as generate \emph{minimal and representative} set of examples that cover all the constructions. 
Think of the whole grammar as a haystack, and we suspect there are a few needles---we cannot promise automatic needle-removal, but instead we help the human oracle to narrow down the search.
% In the end, we need a human to confirm and decide what to do with the needles.

In the following sections, we present our research on testing computational natural language grammars, showcasing two different types of grammar formalisms and testing methods.

\section{Symbolic evaluation of a reductionistic formalism}

Constraint Grammar (CG) \cite{karlsson1995constraint} is a robust and language-independent formalism 
for part-of-speech tagging and shallow parsing. 
A grammar consists of disambiguation rules for initially ambiguous, 
morphologically analysed text: the correct analysis for the sentence 
is attained by removing improper readings from ambiguous words.
Consider the word \emph{wish}: without context, it could be a noun or a verb.
But any English speaker can easily tell that ``a wish'' is a noun.
In CG, we can generalise the observation and write the following rule:

\begin{itemize}
\item[] \texttt{SELECT noun IF (0 noun/verb) (-1 article) ;}
\end{itemize}

Wide-coverage CG grammars, containing some thousands of rules, 
generally achieve very high accuracy: 
thanks to the flexible formalism, new rules can be written 
to address even the rarest phenomena, without compromising the general tendencies.
But there is a downside to this flexibility: large grammars are difficult to maintain
and debug. There is no internal mechanism that prevents rules from contradicting 
each other---the following is a syntactically valid CG grammar, and normal CG compilers will not detect anything strange.

\begin{itemize}
\item[] \texttt{SELECT noun IF (-1 article) ;} \\ \texttt{SELECT verb IF (-1 article) ;}
\end{itemize}

Apart from obvious cases as above, with two rules saying contradicting truths about the language, there can be less direct conflicts between the rules. Take the following example:

\begin{itemize}
\item[] \texttt{REMOVE article ;} \\ \texttt{SELECT noun IF (-1 article) ;}
\end{itemize}

\noindent The first rule removes all articles unconditionally, thus rendering the second rule invalid: it may never apply, because its condition is never matched.

In real life, these conflicts can be much more subtle, and appear far apart from each other. %Grammar writers keeping track is infeasible.
The common way of testing grammars is to apply them into some test corpus, with a gold standard, and gather statistics how often the rules apply. While this method can reveal that \texttt{SELECT noun IF (-1 article)} never applied, it cannot tell whether it is just because no sentence in the test corpus happened to trigger the contextual test, or whether some other rule before it made it impossible to ever apply. 

We use a method called \emph{symbolic evaluation}: in high level terms, we pretend to apply the grammar to every possible input, and track the consequences of each decision. 
The rules become interconnected, and we can find the reason for a conflict. This allows us to answer questions such as ``given this set of 50 rules, is there an input that goes through the first 49 rules and still triggers the 50th rule?''

%This method can give the grammar writer insights about the rules within a grammar. However, it does not tell whether some rule is \emph{useful} for real text, i.e. if it applies often, or if the result of the disambiguation makes any sense. This method is good to complement with other methods: corpus to find out if it applies often, and human grammarian to figure out if it makes sense.


\section{Test case generation for a generative formalism}

Grammatical Framework (GF) \cite{ranta2011gfbook} is a generative, multilingual grammar formalism with a high level of abstraction.
As opposed to CG, where the rules \emph{reduce} the number of possible readings, GF and other generative formalisms use rules to \emph{generate} a language from scratch. In other words, the language corresponding to a grammar is defined as the set of strings that the grammar rules generate.

The most typical example of a generative grammar is a context-free grammar. GF is slightly more complex formalism, and we will present it in Section~\ref{sec:gf-intro}. To quickly illustrate the properties of a generative grammar, we use the following context-free grammar $G$:

\begin{verbatim}
              Sentence ::= Noun Verb | Noun "says that" Sentence
     G =      Noun     ::= "John" | "Mary"
              Verb     ::= "eats" | "sleeps"
\end{verbatim}

We call \texttt{Sentence} as the start category: all valid constructions in the language $G$ are sentences. There are two ways of forming sentences: one is to combine a \texttt{Noun} and a \texttt{Verb}, which results in the finite set \{\emph{John eats, John sleeps, Mary eats, Mary sleeps}\}. However, the second way of constructing a sentence is recursive, resulting in an infinite amount of sentences: \emph{John says that Mary says that \dots Mary eats}.

Again, the grammars we are really interested in are much larger, and the GF formalism has more complex machinery in place, such as taking care of agreement, i.e. allowing \emph{John sleeps} and \emph{I sleep}, but rejecting \emph{*I sleeps} and \emph{*John sleep}. But the basic mechanism is the same: a formal, rather compact description that generates a potentially infinite set of sentences.

It is fully possible to write a ``bad'' generative grammar, in the sense that it produces sentences that any English speaker would deem grammatically incorrect. But there is no notion of internal inconsistency, like there was in CG---the question is simply ``does this grammar describe the English language adequately?''.
In order to test this, we must go beyond the grammar itself, and find a human oracle to answer the question.
But here we face a problem: what kind of subset of the grammar should we test? How do we know if we have tested enough?

The problem is relevant for any kind of software testing, and we use existing techniques of test case generation, applied to the GF formalism. For each language, we generate a minimal and representative set of example sentences, which we give to native or fluent speakers to judge.

\section{Structure of this thesis}

The thesis is divided in two parts: one about Constraint Grammar (Chapters~\ref{chapterCGSAT} and \ref{chapterCGana}) and other about Grammatical Framework (Chapter~\ref{chapterGFtest}). The core of Chapters~\ref{chapterCGSAT}  and \ref{chapterCGana} is based on two articles, which present the SAT-encoding and its application to grammar testing: ``Constraint Grammar as a SAT problem'' \cite{listenmaa_claessen2015} 
and ``Analysing Constraint Grammars with a SAT-solver'' \cite{listenmaa_claessen2016}. 
A third article, ``Cleaning up the Basque grammar: a work in progress'' \cite{listenmaa2017basque}, presents a practical application of the method on a Basque CG, and is included in the evaluation section.
A fourth article, ``Exploring the Expressivity of Constraint Grammar'' \cite{kokke2017expressivity}, presents a novel use for the SAT-encoding.

Chapter~\ref{chapterGFtest} is an extension of one article, ``Automatic test suite generation for PMCFG grammars'' \cite{listenmaa_claessen2018}. 

For both parts, some of the content has been updated since the initial publication; in addition, the implementation is described in much more detail. The thesis is structured as a stand-alone read; however, a reader who is familiar with the background, may well skip Chapter~\ref{chapterBackground}.

Chapter~\ref{chapterBackground} presents a general introduction to both software testing and computational grammars, aimed for a reader who is unfamiliar with the topics.
Chapter~\ref{chapterCGSAT} introduces the SAT-encoding of Constraint Grammar, with two different schemes for conflict handling. The chapter is followed by an appendix, which describes the SAT-encoding of every rule in detail.
Chapter~\ref{chapterCGana}
describes the method of analysing CG by using symbolic evaluation, along with evaluation on three different grammars.
Chapter~\ref{chapterGFtest} presents the method of generating test cases for GF grammars, along with evaluation on a large-scale resource grammar and a couple of smaller grammars.
Chapter~\ref{chapterConclusions} concludes the thesis.


\section{Contributions of the author}

The three articles which present the general methods \cite{listenmaa_claessen2015,listenmaa_claessen2016,listenmaa_claessen2018} are co-written by the author and Koen Claessen.
In general, the ideas behind the publications were joint effort.
For the first article \cite{listenmaa_claessen2015}, all of the implementation of SAT-CG is by the author,
and all of the library SAT+ by Koen Claessen.
%I was mainly responsible of writing the article. 
The version appearing in this monograph, Chapter~\ref{chapterCGSAT}, is thoroughly rewritten by the author since the initial publication.

For the second article \cite{listenmaa_claessen2016}, the author of this work was in charge of all the implementation, except for the ambiguity class constraints, which were written by Koen Claessen. 
%Writing the article was joint effort of both authors. 
The version appearing in Chapter~\ref{chapterCGana} is to large extent the same as the original article, with added detail and examples throughout the chapter. The chapter incorporates material from two additional articles: ``Exploring the Expressivity of Constraint Grammar'' \cite{kokke2017expressivity}, which is joint work between the author and Wen Kokke, and ``Cleaning up the Basque grammar: a work in progress'' \cite{listenmaa2017basque}, co-authored with Jose Maria Arriola, Itziar Aduriz and Eckhard Bick. In \cite{kokke2017expressivity}, the initial idea was developed together, implementation was split in approximately half, and writing was done together. In \cite{listenmaa2017basque}, the author was in a main role in both the implementation and writing.

The work in the third article \cite{listenmaa_claessen2018} was joint effort: both authors participated in all parts of planning and implementation. Both the original article and the version appearing in this monograph, Chapter~\ref{chapterGFtest}, was mainly written by the author.


%The two remaining articles about CG were written with different coauthors: \cite{listenmaa2017basque} is a practical application of the method where the author was in charge with the experiments, and \cite{kokke2017expressivity} is a rather theoretical side trip, w a smaller role of the implementation, but actively participating in the ideas behind the work.