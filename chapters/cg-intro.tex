\def\t#1{\texttt{#1}}

\section{Introduction to CG}
\label{sec:cg-intro}
 

Constraint Grammar (CG) is a formalism for 
disambiguating morphologically analysed text. 
It was first introduced by Fred Karlsson  
\cite{karlsson1990cgp,karlsson1995constraint}, and has been used for many tasks in
computational linguistics, such as part-of-speech tagging, surface syntax and
machine translation \cite{bick2011}.
CG-based taggers are reported to achieve F-scores of over 99 \% for morphological disambiguation, 
and around 95-97 \% for syntactic analysis \cite{bick2000palavras,bick2003hybridCG_PSG,bick2006spanish}.
CG disambiguates morphologically analysed input by using
constraint rules which can select or remove a potential analysis (called \emph{reading})
for a target word, depending on the context words around it. 
Together these rules disambiguate the whole text.

%CG provides a lightweight framework where already a
%moderate amount of rules can be useful (e.g. \cite{lene_trond2011} with 115
%rules), although high quality requires in general thousands of rules.
%\cite{bick_barebones} experiments using CG without a morphological analyser, and reports F-scores over 90 \%.

In the example below, we show an initially ambiguous sentence ``the bear
sleeps''. 
It contains three word forms, such as \t{"<bear>"}, each followed by its \emph{readings}.
A reading contains one lemma, such as \t{"bear"}, and a list of morphological tags, such as \t{noun sg}.
%Additional lemmas within one word form, such as clitic pronouns, are represented as \emph{subreadings}; each indented one more tab.
A word form together with its readings is called a \emph{cohort}. A cohort is ambiguous, if it contains more than one reading.

\begin{figure}[h]
\centering
\ttfamily
\begin{tabular}{p{0.6cm} l  p{0.6cm} l}
"<the>"  &                & "<sleeps>"        \\
    & "the" det def       &     & "sleep" noun pl \\
"<bear>" &                &     & "sleep" verb pres p3 sg \\
    & "bear" noun sg      & "<.>"                   \\
    & "bear" verb pres    &     & "." sent          \\
    & "bear" verb inf \\
\end{tabular}
\label{fig:theBearSleeps}
\caption{Ambiguous sentence {\em the bear sleeps.}}
\end{figure}


\noindent We can disambiguate this sentence with two rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item \texttt{REMOVE verb IF (-1 det)}
  `Remove verb after determiner'
\item  \texttt{REMOVE noun IF (-1 noun)}
  `Remove noun after noun'
\end{enumerate}

\noindent Rule 1 matches the word \emph{bear}: it is tagged as verb and is
preceded by a determiner. The rule removes both verb readings from
\emph{bear}, leaving it with an unambiguous analysis \texttt{noun sg}.
Rule 2 is applied to the word \emph{sleeps}, and it removes the noun
reading. The finished analysis is shown below:

\begin{itemize}
\item[] \begin{verbatim}
"<the>"
        "the" det def
"<bear>"
        "bear" noun sg
"<sleeps>"
        "sleep" verb pres p3 sg
\end{verbatim}
\end{itemize}

It is also possible to add syntactic tags and dependency structure within CG. 
After disambiguating \emph{bear} by part of speech (noun), it
remains ambiguous whether it is a subject, object, adverbial or
any other possible syntactic role.
Disambiguating the syntactic role can be done in several ways. One option is to get both
syntactic and morphological analyses from the initial parser---the analysis
for \emph{bear} would look like the following:

\begin{itemize}
\item[] \begin{verbatim}
"<bear>"
        "bear" noun sg           @Subj
        "bear" noun sg           @Obj
        "bear" verb pres         @Pred
        "bear" verb inf 
\end{verbatim}
\end{itemize}

\noindent Morphological disambiguation rules would
resolve \emph{bear} into a noun, and syntactic disambiguation rules
would be applied later, to resolve \emph{bear}-noun into subject or object.

Alternatively, one can have the initial parser return only
morphological tags, and use CG rules that map a syntactic tag to a
reading (or a reading to a cohort).
%The rules to add work similarly to the rules to remove and select:
%if a target matches the context, then the new tag is added.
These rules are usually run after the morphological disambiguation is
finished.
The following rules could be applied to the example sentence. The
letter `C' after the position marker means \emph{careful} context: e.g. 
\texttt{(-1C noun)} requires the previous word to be unambiguously
noun. Such a rule would not fire if the previous word has any other
reading in addition to the noun reading.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item[] \texttt{MAP @Pred IF (0C verb) (-1C noun)}
\item[] \texttt{MAP @Subj IF (0C noun) (1C verb)}
\end{itemize}

\noindent With both of the strategies, we would end up with the
following output:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item[] \begin{verbatim}
"<the>"
        "the" det def            
"<bear>"
        "bear" noun sg           @Subj 
"<sleeps>"
        "sleep" verb pres p3 sg  @Pred

\end{verbatim}
\end{itemize}

The syntactic tags can also indicate some dependency structure. For
instance, \emph{the} could get a tag such as
\texttt{DN\textgreater}, to indicate that it is a determiner whose
head is to the right. A phrase such as \emph{the little dog} could get
tagged as  \emph{the}$_{\text{\sc @dn>}}$ \emph{little}$_{\text{\sc @an>}}$ \emph{dog}$_{\text{\sc @head-n}}$.
Using \textsc{\textless{}tag} and \textsc{tag\textgreater} can identify
chunks, as long as they are not disconnected.

With the introduction of VISL CG-3 \cite{vislcg3,bick2015}, 
it is possible to add full-fledged dependency relations, 
%: number the words in the sentence and for each set a parent,
such as \texttt{"<the>" "the" det def  \textbf{IND=1 PARENT=2}}. 
However, 
%these features are recent and not used in many of the grammars written in the community.
for the remainder of this introduction, we will illustrate the examples with the 
most basic operations, that is, disambiguating morphological tags.
The syntactic operations are not fundamentally
different from morphological: the rules describe an \emph{operation}
performed on a \emph{target}, conditional on a \emph{context}.


\section{Related work}

CG is one in the family of shallow and reductionist grammar formalisms. In this section, we briefly describe other formalisms of the same family, and provide a bit of historical context. In-depth details about CG in particular are presented in Section~\ref{sec:properties}.

Disambiguation using constraint rules dates back to 1960s and 1970s---the closest system to modern CG was Taggit \cite{taggit}, which was used to tag the Brown Corpus.
Karlsson \cite{karlsson1995constraint} lists various approaches to the disambiguation 
problem, including manual intervention, statistical optimisation, unification and Lambek calculus. 
For disambiguation rules based on local constraints, Karlsson mentions \cite{hindle1989disamrules,herz1991local}.

%Hindle (1983, 1989) uses 350 disambiguation rules, constituting half of the rules of Fidditch. Shieber (1983) postulated disambiguation rules for solving attachment ambiguities in connection with a shift-reduce parser. Herz – Rimon (1990) use local constraints disallowing certain sequences of tags from being in the near context of a target word with designated features. The local constraints are retrieved from a context-free grammar of the language.

CG itself was introduced in 1990. Around the same time, a related formalism was proposed: 
finite-state parsing and disambiguation system using constraint rules \cite{koskenniemi90}, which was later named Finite-State Intersection Grammar (FSIG) in \cite{piitulainen1995}. 
Like CG, a FSIG grammar contains a set of rules which remove impossible readings based on contextual tests, 
in a parallel manner: a sentence must satisfy all individual rules in a given FSIG. 
Due to these similarities, the name Parallel Constraint Grammar was also suggested \cite{koskenniemi97}.
However, FSIG rules usually target syntactic phenomena, and the rules allow for more expressive contextual tests than CG. 
In this thesis, we use the name FSIG to refer to the framework that is aimed at producing a full syntactic analysis with the more expressive rules, 
and PCG to describe any CG-variant which happens to be parallel.
Thus, we will call implementations such as \cite{voutilainen1994designing} an \emph{FSIG grammar} and a program that parses it an \emph{FSIG parser}.
Conversely, the CG parser in \cite{listenmaa_claessen2015} 
and the ``CG-like'' system in \cite{lager98} are instances of PCG.
We return to the comparison with FSIG in Sections~\ref{sec:ordering}~and~\ref{sec:expressivity}.


Brill tagging \cite{brill1995} is based on transformation rules: the starting point of 
an analysis is just one tag, the most common one, and subsequent rule applications transform 
one tag into another, based on contextual tests. 
Like CG, Brill tagging is known to be efficient and accurate. The contextual tests are very similar; \cite{lager01transformation} 
has automatically learned both Brill rules and CG rules, using the same system.

Similar ideas to CG have been also explored in other frameworks, such as finite-state automata \cite{gross1997local,grana2003fst},
%, such as Local grammars \cite{gross1997local} and finite-state contextual rules \cite{grana2003fst},
logic programming \cite{oflazer97votingconstraints,lager98}, 
constraint satisfaction \cite{padro1996csp}, 
and dependency syntax \cite{tapanainen97fdg}. 
 In addition, there are a number of reimplementations of CG using finite-state methods \cite{yli-jyra2011cg_engine,hulden2011cg_engine,peltonen2011}. 


%\todo{All the other things that the standard CG-related papers/theses cite}


\section{Properties of Constraint Grammar}\label{sec:properties}

Karlsson \cite{karlsson1995constraint} lists 24 design principles and describes
related work at the time of writing.
Here we summarise a set of main features, and relate CG to the developments in grammar formalism since the initial description.

CG is a \emph{reductionistic} system: the analysis starts from a list of alternatives,
and removes those which are impossible or improbable.
CG is designed primarily for analysis, not generation; its task is 
to give correct analyses to the words in given sentences,
not to describe a language as a collection of ``all and only the grammatical sentences''.

The syntax is decidedly \emph{shallow}: the rules do not aim to
describe all aspects of an abstract phenomenon such as noun phrase; 
rather, each rule describes bits and pieces with concrete conditions.
The rules are self-contained and mutually independent---this makes it 
easy to add exceptions, and exceptions to exceptions, without 
changing the more general rules.

There are different takes on how \emph{deterministic} the rules are.
The current state-of-the-art CG parser VISL CG-3 executes the rules strictly 
based on the order of appearance, but there are other implementations which 
apply their own heuristics, or remove the ordering completely, 
applying the rules in parallel. %such as in finite-state or logic-based implementations. 

In addition, we will discuss the \emph{expressivity} of CG. 
%As said, CG is not a generative grammar, and thus cannot be placed in the Chomsky hierarchy.
We present earlier work on defining expressivity in CG and similar formalisms,
and suggest a way to emulate generation; this would allow us to place CG in the Chomsky hierarchy, and better relate to other grammar formalisms.
%aim to provide parallels in what kind of output it can produce.

\subsection{Reductionistic}\label{reductionist-vs.licencing}

CG analysis starts from a set of alternative readings, given by a morphological analyser,
and eliminates the impossible or improbable ones using constraint rules. 
The remaining analyses are assumed to be correct; that
is, everything that is not explicitly eliminated, is allowed. 
This kind of system is called \emph{reductionistic}. It is contrasted 
with a \emph{licencing} (or \emph{generative}) system, where all constructions must
be explicitly allowed, otherwise they are illegal. 
An empty reductionist grammar will accept any string, whereas an empty
licencing grammar will accept no string.

%\cite{karlsson1995constraint} argues that this reductionistic model is 
%closer to a psycholinguistic model, because of less processing load.


% \paragraph{From Fred's 1995 book}
% \begin{quote}
% Notice, in passing, that the overall reductionistic set-up gives an
% interesting psycholinguistic prediction. In terms of processing load,
% less processing is required under Constraint Grammar to turn out an
% ambiguous and therefore unclear analysis than to turn out a
% disambiguated and therefore structurally clear analysis. In our
% opinion, this seems to be a reasonable psycholinguistic hypothesis a priori.
% Mental effort is needed for achieving clarity, precision, and maximal
% information. Less efforts imply (retention of) unclarity and
% ambiguity, i.e. information decrease. In several types of parsers, 
% rule applications create rather than discard ambiguities: the more
% processing, the less unambiguous information.
% \end{quote}

According to the initial specification, CG is meant for analysis---Karlsson \cite{karlsson1995constraint} does not see it fit for generation:

\begin{quote}
%It would be optimal if the formalism were general and abstract enough [--] to be used both for sentence analysis and sentence generation. 
%Constraint Grammar takes no explicit stand on this issue. 
In practice, however, constraints are geared towards parsing, and Constraint Grammars are analysis grammars. It remains to be demonstrated that full-scale syntax can be done by one and the same reversible grammar.
\end{quote}
\noindent Two decades later, Eckhard Bick and Tino Didriksen \cite{bick2015} 
describe CG as ``a declarative whole of contextual possibilities and impossibilities 
for a language or genre'', which is nevertheless implemented in a
low-level way: selecting and removing readings from individual words,
without explicit connection between the rules. Bick and Didriksen
argue that as a side effect, the rules actually manage to describe language.

We return to the questions of generation and expressivity in section~\ref{sec:expressivity}. 
In chapter~\ref{chapterCGana}, we revisit these questions in the context of analysing CGs.

 
\paragraph{No enforcement of grammaticality} 

% \cite{karlsson1995constraint} states explicitly:

% \begin{quote}
% The foremost task of a parsing-oriented grammar is rather to aid in parsing every input than to define the notion ”grammatically correct sentence”, or to describe ”all and only the grammatical sentences”.
% \end{quote}


CG does not define sequences as \emph{grammatically correct}: it simply aims to add structure to any input.
%Beside the lack of long-distance dependencies, e
Even local phenomena, such as gender agreement, is not necessarily enforced.
However, this is often desirable for practical purposes, because it makes the grammar more robust.
Let us illustrate with an example in Swedish.

\begin{itemize}
\item[] \begin{verbatim}
"<en>"
        "en" det indef utr sg
        "en" noun utr sg 
"<bord>"
        "bord" noun neutr sg
\end{verbatim}
\end{itemize}

The first word, \emph{en}, is ambiguous between the indefinite determiner for
\emph{utrum} gender  (masculine and feminine collapsed into one), or the noun `juniper'.
The second word, \emph{bord}, is a neuter noun (`table')---we can assume that the
most likely meaning is ``a table'' instead of ``juniper table''\footnote{Compound words are written without a space: ``juniper table'' would be \emph{enbord}.}, but the writer has mistaken the gender of `table'. The correct form, which also would not be ambiguous, is ``ett bord''.

%CG allows the rules to be as fine-grained or broad as we want: 
CG rules can be as fine-grained or broad as we want: 
\texttt{SELECT det IF (1 noun) (0 noun)} would match any determiner and any noun, 
and successfully disambiguate \emph{en} in this case. 
We can also enforce grammaticality by writing \texttt{SELECT (det~utr~sg) IF (1~noun~utr~sg) (0~noun~utr~sg)}%\footnote{the counterpart neuter determiner is not ambiguous with noun, so \texttt{SELECT det neutr sg IF (1 noun neutr sg) (0 noun)} would be redundant. Though it can be ambiguous with the numeral `1'. So actually maybe it's good anyway.}
. In that case, nothing in the grammatically incorrect phrase matches, and it is left ambiguous.


%On the other hand, we can introduce such fine-graind rules in order to pick the right analysis, for instance, if a certain modifier is underspecified. If we analyse Spanish text \emph{la casa grande}, where \emph{grande} is underspecified for gender, then having a rule that picks the feminine adjective after a feminine article and feminine noun makes our analysis of \emph{grande} more accurate. 
% a certain adjective is underspecified for gender but its head noun is specified, then we can pick the right gender for the adjective.

The previous example illustrates that the notions of \emph{accept} and \emph{reject} are not clear: if agreement was enforced
by enumerating only legal combinations, the grammar would just refuse to disambiguate
an ungrammatical sequence and leave all tags intact---in that case,
its performance would not differ from a grammar that simply does not have many rules.
%The difference lies in that a generative grammar starts building structure from scratch---there are no errors in an empty tree, and i
%In contrast, CG has to assign structure for any input, including grammatically incorrect.
The sequence \emph{en}$_{\textsc{Det Utr}}$~\emph{bord}$_{\textsc{N Neutr}}$ is undeniably ungrammatical; however, the alternative \emph{en}$_{\textsc{N Utr}}$~\emph{bord}$_{\textsc{N Neutr}}$  would be even ``more wrong'' for the purpose of analysing the text.

% If we define \emph{grammatical sequences} to be sequences of POS tags,
% then the question is somewhat easier to answer: \verb!<det><utr> <noun><neutr>! is ungrammatical, and CG does not raise an alarm that there is something wrong; hence we can say that CG ``accepts'' ungrammatical sequences. However, the alternative \verb!<noun><utr>! \verb!<noun><neutr>! would be even ``more wrong'' for the purpose of analysing the text; same goes for the option with no disambiguation at all.

%As \cite{karlsson1995constraint} states, CG is not designed for defining grammaticality, but rather providing analyses for real-life texts.

% Of course, the latter point can be made of any reductionist system, not only CG.

\subsection{Shallow syntax}\label{shallow-syntax}

CG can be described as a shallow formalism for various reasons. 
The analysis is limited to concrete words; no invisible structure is postulated.
CG rules usually operate on a low level of abstraction, and may target individual lexemes or word forms.
% instead of one rule to describe the possible structure of noun phrase,
%CG would include many small rules each describing one possible kind of noun phrase.
Finally, the rules are independent: each rule describes its own piece of a phenomenon, and there is no inbuilt mechanism to guarantee that the rules are consistent with each other.



\paragraph{Low hierarchy}


We saw that the analysis starts from a list of alternatives for each word, and proceeds by eliminating impossible or unlikely candidates.
These alternatives may include syntactic labels as well as morphological, but there are no invisible components in the analysis: all labels attach to a concrete token in the phrase.
Syntactic or dependency labels are functionally the same as morphological: for example, \texttt{@Subj} is just one more tag in a reading.
In fact, it is possible to add even more data to the readings, either from the initial morphological analysis, or with suitable post-processing. One could include semantic roles, or any other information that a lexicon may provide, such as animacy or frequency.
As a result, readings may look like the following:

\begin{itemize}
\item[] 
\begin{verbatim}
"<cat>"
        "cat" noun sg   @Subj £Agent §Animal
        "cat" verb pres @Pred §Computer §Rare
\end{verbatim}
\end{itemize}

\noindent Moreover, any of these levels can be accessed in the same rule: e.g. \texttt{SELECT noun IF (-1 "cat" §Animal) (NOT 0 §Rare)}. 
In other words, we may use syntactic or semantic features to help in morphological disambiguation, and vice versa.

This lack of deep structure is intentional in the design of CG.
Karlsson \cite{karlsson1995constraint} justifies the choice with a number of
theoretical principles: that language is open-ended and grammars are
bound to leak, and that necessary information for the syntactic analysis
comes from the morphology, hence morphology is ``the cornerstone of
syntax''. 
In addition, Karlsson \cite{karlsson1995constraint} argues CG to be a reasonable 
psycholinguistic hypothesis: 
in generative formalisms, rule applications create ambiguities,
whereas in CG, rule applications reduce ambiguity. This means less processing load 
in order to achieve clarity. 

% indeed, CG has been used in psycholinguistic research.
%\todo{cite some paper by CLEAR people} find CG appealing especially because all levels are accessible at the same time, from lexicon and morphology to semantics.

% In a CG, one has access to all levels at the same time,
% ranging from morphology to syntax or dependency labels, or even
% semantics, if one wants to introduce semantic roles in the
% tagset. One can have a rule such as \texttt{REMOVE ... IF (+1 "bear" <noun>
%   @Object §Patient)}, followed by a second rule which only considers the POS of the
% context word.


% \cite{karlsson1995constraint}
% \begin{quote}
% For example, ordinary phrase structure rules for English NPs would state that a determiner is followed by certain optional premodifiers plus an obligatory noun. A consequence of this is that a determiner cannot be followed by a finite or infinitive verb form, without an intervening noun. In Constraint Grammar, a constraint could be postulated to this effect, discarding the verbal readings of the word-form bank in expressions like the bank, the old bank.
% \end{quote}



\paragraph{Low abstraction} 

As for the level of abstraction, CG rules lie somewhere between 
traditional grammar formalisms, such as HPSG, and purely statistical methods.
The rules for morphological disambiguation do not usually handle long-distance dependencies; 
they operate on a unit of a few words, nor do they abstract away from surface features such as word order.

The rule that removes a verb reading after a determiner
does not describe the whole abstract category ``noun phrase''; 
we need a second rule to remove a verb after a determiner and an adjective 
(\emph{the happy bear})  or an adjectival phrase (\emph{the very happy bear}).
There is a template feature, present already in CG-1, which allows to list alternatives under a single name, such as ``\t{det~n}'', ``\t{det~adj~n}'' and ``\t{adj~n}'' under \t{NP}. But this is just syntactic sugar: one rule with a condition \t{IF -1 NP} is expanded into three rules, with the respective conditions \t{IF~(-2~det)~(-1~n)}, \t{IF~(-3~det)~(-2~adj)~(-1~n)} and \t{IF~(-2~adj)~(-1~n)}.

Finally, some rules may be purely lexical, such as \t{REMOVE v IF (-1 "bear")}, and some rules may target the whole class of nouns, or something in between, such as all finite verb forms. These decisions are motivated by what is needed for disambiguating real-life texts, rather than formulating the most elegant and concise rules possible.
%A phenomenon that is expressed with one big rule in a deep formalism is split into many low-level rules.



\paragraph{Independence of rules}
The rules are self-contained and independent.
On the one hand, this provides no guarantee that a grammar is internally consistent.
On the other hand, these features provide flexibility that is hard to mimic by a deeper formalism.
As we have seen in the previous sections, rules can target individual words
% (select \texttt{noun} if the word form is \emph{bear})
or other properties that are not generalisable to a whole word class,
such as verbs that express cognitive processes.
Introducing a subset of verbs, even if they are used only in one rule,
is very cheap and does not create a complicated taxonomy of different verb types.

Most importantly, the independence of rules makes CG highly robust.
If one of the words is unknown or misspelt, a generative grammar would fail to produce any analysis. 
CG would, at worst, just leave that part ambiguous, and do as good a job it can elsewhere in the sentence.

% We can introduce a similar granularity in other systems, for instance,
% a grammar in Grammatical Framework, which would include the functions
% \texttt{GenVP} for whichever verbs and \texttt{CogVP} for cognitive verbs:


% \begin{verbatim}
% GenVP : V2    -> NP -> VP ;
% CogVP : CogV2 -> NP -> VP ;
% \end{verbatim}

% In order to parse as much text as without the subcategorisation, the
% grammar writer would have to add \texttt{CogV*} counterparts all
% functions that exist for \texttt{V*}, which duplicates code; or add
% coercion functions from  \texttt{CogV*} to \texttt{V*}, which increases ambiguity.
% In case of multiple parses, likely the one with more information
% would be preferred, but the CG solution gives more flexibility. If a
% rule which matches \texttt{CogV} is introduced before (more on
% ordering in the following section) a rule which matches \texttt{V}, it
% is applied before, and vice versa. This allows for very fine control,
% for example combining the semantic properties of the verb and the
% animacy of the arguments.

%  From an engineering point of view, even an incomplete CG can
% already disambiguate a lot, whereas an incomplete phrase structure
% grammar is nearly useless.

\subsection{Ordering and execution of the rules}\label{sec:ordering}

The previous properties of Constraint Grammar formalism and rules were specified in \cite{karlsson1995constraint}, and retained in further implementations. 
However, in the two decades following the initial specification, 
several independent implementations have experimented with different ordering
schemes. In the present section, we describe the different parameters of ordering and execution: \emph{strict vs. heuristic}, and \emph{sequential vs. parallel}.
% Even Karlsson's original specifications differ as for the \emph{ordering} of the rules:
% \cite{karlsson1990cgp} describes the CGP as executing the rules strictly in the order in which they appear:
% \begin{quote}
% There may be several mapping statements for the same morphological feature(s), e.g. ``N NOM''. Mapping statements with more narrowly specified contexts have precedence over more general statements. 
% In the present implementation of CGP, the mapping statements apply in plain linear order. 
% \end{quote}
%
% but \cite{karlsson1995constraint} states on page 17:
%
% \begin{quote} 
% Each single statement is true when examined in isolation, either absolutely or with some degree of certainty, depending upon how careful the grammar writer has been. Furthermore, disregarding morphosyntactic mappings, the constraints are \emph{unordered}.
% \end{quote}
%
% The execution of the rules depends on the following parameters: strict vs. heuristic ordering, and sequential vs. parallel execution. These parameters may combine freely: strict and sequential, strict and parallel, heuristic and sequential, or heuristic and parallel. 
Throughout the section, we will apply the rules to the following ambiguous passage, ``{\em What question is that}'':

\begin{tabular}{p{0.6cm} l  p{0.6cm} l p{0.6cm} l p{0.6cm} l}
\t{"<what>"}     &                      &  \t{"<question>"}        &  & \t{"<is>"}         & & \t{"<that>"} \\
                 & \t{"what" det}       & &      \t{"question" noun}  &  &    \t{"be" verb}  & &    \t{"that" det}  \\
                 & \t{"what" pron}      & &      \t{"question" verb}  &  &                   & &    \t{"that" rel} \\
\end{tabular}

% \begin{tabular}{p{0.6cm} l  p{0.6cm} l}
% \t{"<what>"}     &                      &  \t{"<is>"}           & \\
%                  & \t{"what" det}       & &      \t{"be" verb}    \\
%                  & \t{"what" pron}      &                       & \\
% \t{"<question>"} &                      & \t{"<that>"}         & \\
%                  & \t{"question" noun}  & &      \t{"that" det}   \\
%                  & \t{"question" verb}  & &      \t{"that" rel}
% \end{tabular}





\def\satcgMax{SAT-CG\textsubscript{Max}}
\def\satcgOrd{SAT-CG\textsubscript{Ord}}
\def\noncg#1{{\em \color{gray} #1}}

\begin{table}[h!]
\centering

  \begin{tabular}{r | p{2.5cm} | p{3.5 cm} | p{3.5cm}}
           & \textbf{Strict} & \textbf{Heuristic} & \textbf{Unordered} \\ \hline
\textbf{Sequential}
           & CG-1 \cite{karlsson1990cgp}   
                             & CG-2 \cite{tapanainen1996} % (Tapanainen1996, p. 34
                                                  & --           \\ 
           & VISL CG-3  \cite{vislcg3}      
                             & Weighted CG-3 \cite{pirinen2015} & \\ 
           & Peltonen 2011 \cite{peltonen2011}  &  &\\ %Peltonen2011, p. 80
           & Yli-Jyrä 2011 \cite{yli-jyra2011cg_engine}  & & \\ 
           & Huldén 2011 \cite{hulden2011cg_engine} & & \\ \hline
\textbf{Parallel}
           & \satcgOrd       & \satcgMax          & \noncg{Lager 1998 \cite{lager98}} \\ 
           &                 & \noncg{FSIG (Voutilainen) \cite{voutilainen1994designing}} 
                                                  & \noncg{FSIG (Koskenniemi) \cite{koskenniemi90}} \\
           &                 & \noncg{Voting constraints \cite{oflazer97votingconstraints}}  \\


  \end{tabular}
  \caption{Combinations of rule ordering and execution strategy.}
  \label{table:nelikentta}
\end{table}

\paragraph{Strict vs. heuristic}

aka. ``In which order are the rules applied to a single cohort?''

An implementation with \emph{strict order} applies each rule in the order in 
which they appear in the file. If a grammar contains the rules \t{REMOVE v IF (-1 det)} and \t{REMOVE n IF (-1 pron)} in the given order, the rule that removes the verb reading in \emph{question} will be applied first. After it has finished, there are no verb readings available anymore for the second rule to fire.
					
How do we know which rule is the right one? There can be many rules that fit the context, but we choose the one that just happens to appear first in the rule file. 
A common design pattern is to place rules with a long list of conditions first; 
only if they do not apply, then try a rule with less conditions. For a similar effect,
a \emph{careful mode} may be used: ``remove verb after \emph{unambiguous} determiner'' 
would not fire on the first round, but it would wait for other rules to clarify the
status of \emph{what}.

% More careful rules are usually placed first, followed by stronger rules---see the pattern below.

% \begin{verbatim}					
% SELECT <rare analysis> IF <rare condition> ;
% ...
% REMOVE <rare analysis> ;
% \end{verbatim}
% If a rare condition is met, the rare analysis is selected, and since it will be the only analysis, it will be protected from the remove rule. If the rare condition is not met, the rare analysis is removed.
					
An alternative solution to a strict order is to use a \emph{heuristic order}: when disambiguating a particular word, find the rule that has the longest and most detailed match. Now, assume that there is a rule with a longer context, such as \t{SELECT n IF (-1 det) (1 v)}, even if this rule appears last in the file, it would be preferred to the shorter rules, because it is a more exact match.
There are also methods that use explicit weights to favour certain rules, such as \cite{pirinen2015} for CG, and \cite{voutilainen1994designing,oflazer97votingconstraints,silfverberg2009conflict} for related formalisms. 
					
Both methods have their strengths and weaknesses. A strict order is more predictable, but it also means that the grammar writers need to pay more thought to rule interaction. A heuristic order frees the grammar writer from finding an optimal order, but it can give unexpected results, which are harder to debug.
As for major CG implementations, CG-1 \cite{karlsson1990cgp} and VISL CG-3 \cite{vislcg3} follow the strict scheme, whereas CG-2 \cite{tapanainen1996} is heuristic\footnote{Note that CG-2 is heuristic \emph{within sections}: the rules in a given section are executed heuristically, but all of them will be applied before any rule in a later section.}.





\paragraph{Sequential vs. parallel}

aka. ``When does the rule application take effect?''
%aka. ``Does the context change immediately after each rule is applied?''

The input sentence can be processed in sequential or parallel manner.
In \emph{sequential execution}, the rules are applied to one word at a time, starting from the beginning of the sentence. The sentence is updated after each application. If the word \emph{what} gets successfully disambiguated as a pronoun, then the word \emph{question} will not match the rule \t{REMOVE v IF (-1 det)}.


In contrast, a \emph{parallel execution} strategy disambiguates all the words at the same 
time, using their initial, ambiguous context. 
%If we want to keep this variable orthogonal to the rule ordering, we may not assume anything about how the rules are ordered.
To give a minimal example, assume we have a single rule, \texttt{REMOVE verb IF (-1 verb)}, and the three words \emph{can can can}, shown below. 
In parallel execution, both \emph{can$_2$} and \emph{can$_3$} lose their verb tag; in sequential only \emph{can$_2$}.

%Parallel and sequential would return a different result for the following passage: 

\begin{figure}[h]
\begin{tabular}{p{0.5cm} l   p{0.5cm} l    p{0.5cm} l}
\t{"<can$_1$>"}      &  & \t{"<can$_2$>"}  & & \t{"<can$_3$>"} & \\
   & \t{"can" noun}  &  & \t{"can" noun} & & \t{"can" noun}   \\
   & \t{"can" verb}  &  & \t{"can" verb} & & \t{"can" verb} 

\end{tabular}
\end{figure}

The question of parallel execution becomes more complicated if 
there are multiple rules that apply for the same context. 
Both \t{REMOVE v IF (-1 det)} and \t{REMOVE n IF (-1 pron)} 
would match \emph{question}, because the original input 
from the morphological analyser contains both determiner and 
pronoun as the preceding word. 
The result depends on various details: shall all the rules also act in parallel?
If we allow rules to be ordered, then the result will not be any different from 
the same grammar in sequential execution; that is, the later rule (later by any metric) 
will not apply. 
The only difference is the reason why not: ``context does not match'' in sequential,  
and ``do not remove the last reading'' in parallel.

However, usually parallel execution is combined with \emph{unordered} rules.
In order to express the result of these two rules in an unordered scheme, we need a concept 
that has not been discussed so far, namely, disjunction:
``the allowed combinations are {\em det}+{\em n} or {\em pron}+{\em v}''. 
If we wanted to keep the purely list-based ontology of CG, but combine it with a parallel 
and unordered execution, then the result would have to inconclusive and keep both readings; 
both cannot be removed because that would leave {\em question} without any readings.
The difference between the list-based and disjunction-based ontologies, corresponding to 
CG and FSIG respectively, is explained with further detail in \cite{lager_nivre01}.


% Which execution scheme is better? For most purposes, sequential execution is preferred. 
% Sequential execution–--together with strict or heuristic rule order---makes the rule take action immediately, and updates the context for the following words. 
% If we know the correct part-of-speech for \emph{what}, then it is easier to disambiguate \emph{question}: there are less possible rules that may apply. 
% Parallel execution may have uses in more marginal cases: for instance, if we have a small rule set and we want to find out if the rules are consistent with each other.
% \cite{koskenniemi90} states the following about FSIG:

%  \begin{quote}
%  Each constraint simply adds something to the discriminating power of the
%  whole grammar. No constraint rule may ever forbid something that would
%  later on be accepted as an exception. This, maybe, puts more strain for
%  the grammar writer but gives us better hope of understanding the grammar
%  we write.
%  \end{quote}

% Perhaps unsurprisingly, nearly all of the existing CG implementations are sequential.
% There are other, parallel formalisms, such as FSIG \cite{koskenniemi90} and the system of voting constraints \cite{oflazer97votingconstraints}; however, for pure CG, all mainstream implementations follow the sequential scheme.
% The CG engine presented in \cite{listenmaa_claessen2015} and in Chapter~\ref{chapterCGSAT} in this thesis is parallel, as well as the ``CG-like'' system in \cite{lager98}, but both of these systems are mostly proof of concept.


Table~\ref{table:nelikentta} shows different systems of the constraint rule family, 
with rule order (strict vs. heuristic) on one axis, 
and execution strategy (sequential vs. parallel) on other. 
Traditional CG implementations are shown in a normal font; other, related systems in cursive font and lighter colour.
\satcgMax and \satcgOrd refer to the systems by the author; they are presented in \cite{listenmaa_claessen2015} and in Chapter~\ref{chapterCGSAT} of this thesis.

% A notable exception is the system proposed in \cite{koskenniemi90}, known by the names 
% Finite-State Intersection Grammar (FSIG) or Parallel Constraint Grammar (PCG) .
% However, since the beginning, (sequential) CG and FSIG were meant for different tasks: 
% FSIG was designed for syntactic parsing, and the rules allow for more expressive contextual tests than CG. 





% The constraint rules are executed in sequence, and the consequences of
% previous rule applications affect the coming rules. In addition, there
% is a default rule, which overrides all other rules: the last reading of
% a word must not be removed.
% 
% If a grammar contains the rules ``select \texttt{noun} after
% \texttt{det}'' and ``select \texttt{verb} after \texttt{det}'',
% whichever occurs first will be applied; after it has selected the noun
% reading and removed the rest, there is no verb reading available anymore
% for the second rule to fire.
% 
% More careful rules are usually placed first, followed by stronger
% rules---see the pattern below. If a rare condition is met, the rare
% analysis is selected, and since it will be the only analysis, it will be
% protected from the remove rule. If the rare condition is not met, the
% rare analysis is removed.
% 
% 
% \begin{verbatim}
% SELECT <rare analysis> IF <rare condition> ;
% ...
% REMOVE <rare analysis> ;
% \end{verbatim}
% 

% \paragraph{Comparison to FSIG} 
% Another example of a reductionist and shallow-syntax formalism is
% Finite-State Intersection Grammar (FSIG) \citep{koskenniemi90}. In the
% formalism, the tagged sequences of words, as well as rules, are modelled
% as finite state automata: for instance, there are two transitions from
% the state \emph{what} to the state \emph{question} in the automaton
% representing \emph{what question is this}. This automaton is intersected with
% a rule automaton, and each path through the resulting automaton
% represents a possible analysis for the whole sequence.
  
% In FSIG, the path that consists of choosing the \texttt{det} arc
% followed by \texttt{verb} arc is ruled out, but other interpretations
% are still valid: \texttt{det+noun}, \texttt{pron+noun} and
% \texttt{pron+verb}. In contrast, applying a rule in CG removes an
% analysis permanently; if the rule removes the verb reading in
% \emph{question}, then we cannot retrieve the \texttt{pron+verb}
% possibility. Therefore, it is important to consider the order of the
% rules. It is possible to operate with \emph{cautious context}, where the
% rule only takes action if the context is unambiguous. For this case, the
% ``remove \texttt{verb} after \texttt{det}'' rule would not fire before
% the word \emph{what} is fully disambiguated as a determiner.


\subsection{Expressivity and complexity of CG}
\label{sec:expressivity}

% Anssin väikkäristä:
% FSIG Is Not a GES Framework FSIG constraints can potentially express any reg- ular set over the given alphabet, being thus similar to right-linear grammars (RLGs). However, there are several reasons why FSIG as a whole is not based on the GES approach, or that it is at least ambiguous:
% • It does not directly define phrase structures,
% • It does not use productions, and
% • It does not derive the sentences with syntactic inferences.


% \item General motivation: why is expressivity interesting in the first place? 
% \item Previous approaches to defining expressivity of CG/CG-like systems
% \item Wild speculation how the work in this thesis could have a possible secondary use, in helping define expressivity.

So far, we have approached CG from a practical and language-oriented point of view. 
But grammar formalisms are interesting also from a computational perspective. 
For the theoretically inclined, making a description of a formalism more precise 
is an end of its own. 
However, even a practically oriented reader should appreciate the applications: 
better understanding of a formalism may lead to novel uses and implementation techniques. 

The standard measure of formal languages is the Chomsky hierarchy \cite{chomsky1956hierarchy}, with its four classes of grammars and languages, in order from most expressive to least expressive: \emph{recursively enumerable} (Type 0), \emph{context-sensitive} (Type 1), \emph{context-free} (Type 2), and \emph{regular} (Type 3). 
The notion of \emph{expressive power}, ``which constructs can we express in the language'' is coupled with \emph{parsing complexity}, ``how much time and memory do we need to parse sentences in the language''; more expressive power corresponds to greater parsing complexity. 

% Intuitively, we can approach expressivity as follows:
% For the purposes of practically oriented language parsing, we may define expressivity as such:

% "it's just that your regex doesn't work as intended. 
% You cannot specify exacly enough, within the limitations of
% regexes, all the conditions when you want a match and when you don't."

However, the Chomsky hierarchy is defined for generative grammar formalisms, hence it is difficult to place a reductionistic formalism such as CG into it. 
Especially the question of expressivity is problematic in the context of CG. 
A given grammar in CG does not describe a language, but a \emph{relation} between an input language and an output language. %: say, $I = \{[\text{"the" }\t{ det def}][\text{"bear" }\t{ noun}, \text{"bear" }\t{ verb}]\}$ and the output is$O =  \{[\text{"the" }\t{ det def}][\text{"bear" }\t{ noun}]\}$.
In addition, we need to consider a number of implementation features: 
as we have learnt, there are many variants of CG, 
with a different take on rule ordering, execution strategy, 
and supported operations. 
Anssi Yli-Jyrä (personal communication) suggests that the 
expressivity should be defined separately for each grammar, and not for the complete formalism. 

In contrast, parsing complexity can be easily defined for a given variant and implementation of CG. 
For instance, Pasi Tapanainen \cite{tapanainen1999phd} and Måns Huldén \cite{hulden2011cg_engine} analyse their CG-2 parser implementations 
in terms of the sentence length $n$; 
the number of rules in the grammar $G$; and the maximal number of different readings per cohort $k$. Tapanainen's parser has the worst-case complexity $O(Gn^3k^2)$, and Huldén's parser $O(Gn^2k^2)$;
even though both systems can run the same CG-2 style grammars\footnote{We assume both systems give the same output for the same input---at least the opposite has not been reported.}, the complexities of the respective parsers are different, due to different implementation techniques. 


% For instance, the CG-2 parser by Pasi Tapanainen \cite{tapanainen1996} 
% and the finite-state parser by Måns Huldén \cite{hulden2011cg_engine} support the same 
% grammars and operations. 
% As for complexity, Tapanainen's CG-2 parser 
% Pasi Tapanainen
% Pasi Tapanainen \cite{tapanainen1999phd} gives a complexity analysis
% for the CG-2 parser, in terms of the sentence length, $n$; 
% the number of rules in the grammar, $G$; and the maximal number of different readings per cohort, $k$.
% Tapanainen shows that the worst-case running time for CG-2 is $O(Gn^3k^2)$. 
%  uses a different implementation technique, and
% achieves the complexity $O(Gn^2k^2)$ for CG engine,
%  which supports the same operations as Tapanainen's CG-2.


% The question ``what can CG express?'' is ill-defined as such: 
% just to start with, there are different variants and implementations of CG, as we have seen in the previous section. 
% Taking that into account, even ``what can sequential and heuristic CG-2 express'' is not specific enough: 
% the answer depends on what is present in the input language, and is different for each grammar (Yli-Jyrä 2016, personal communication).

% As we have learnt, CG is not a generative formalism---there are 
% no production rules nor phrase structure,
% CG just needs a list of alternatives and finds the correct ones among them.


Even if CG is not a generative formalism, the concepts around Chomsky hierarchy play an important role, for instance, when defining what a single rule can express. 
In the following, we look at existing approaches to define the expressivity of reductionist formalisms, 
and suggest a way to emulate generation within (a variant of) CG.

%In addition, there are different variants and implementations of CG: 
%it is not a meaningful question to define the expressivity of ``CG'', but rather CG-1 \cite{karlsson1995constraint}, CG-2 \cite{tapanainen1996}, VISL CG-3 \cite{bick2015}, or any other implementation.

%Take the example of central embedding; a feature known to be beyond the expressivity of regular grammars. %require a context-free grammar. 
%If a given generative grammar would analyse correctly an arbitrary number of central embeddings (e.g. \emph{the dog \emph{the cat bit} died}), this would be evidence that the language goes beyond regular. 
%In order to translate such criterion to CG, we have two questions to ask: 
%\begin{inparaenum}
%\item Can a CG grammar correctly disambiguate a sentence with an arbitrary number of central embeddings?
%\item If such grammar can be written, does it mean that CG is beyond regular?
%\end{inparaenum}


\paragraph{Expressivity of a rule}

Tapanainen \cite{tapanainen1999phd} defines the notion \emph{finite-state constraint language}: a formalism for writing disambiguation rules, 
where the contextual tests are expressed in terms of regular languages.
Indeed, we can verify that in all implementations of CG, 
the contextual tests must be regular: we cannot write a single rule such as 
``remove verb if the previous segment contains only well-nested parentheses''.

Tapanainen gives a hierarchy of four different constraint languages, in order of the expressivity in a single rule: Taggit \cite{taggit} only allows rules to look at 1-2 tokens before or after the target; CG-1 \cite{karlsson1990cgp} allows unlimited context within the window (usually one sentence) and adds operators such as the Kleene star and complement, but it does not allow all combinations, for instance the notion of barrier. According to Tapanainen \cite{tapanainen1999phd}, a contextual test such as $(\Sigma - N)*A(\Sigma - V)*N$\footnote{IF (*1 A BARRIER N LINK *1 N BARRIER V)} is not supported in CG-1; in turn, CG-2 \cite{tapanainen1996} can express it, but not e.g. repeated patterns, such as $(\text{\em Det} \  A \  N)+$. Finally, FSIG \cite{koskenniemi90} allows the full expressivity of regular languages in the contextual tests.

Since CG-3 \cite{vislcg3,bick2015} did not exist at the time, \cite{tapanainen1999phd} does not include it in the hierarchy of the constraint formalisms.
The formalism has added some new constructions to the morphological disambiguation rules, 
such as {\sc cbarrier} for requiring the barrier to be unambiguously tagged, or subreadings, for distinguishing different levels of tags within one reading. 
It is unclear whether these additions make the {\sc remove} and {\sc select} rules of VISL CG-3 strictly more expressive than those of CG-2, or if they just provide syntactic convenience.
\footnote{There are many new rule types in VISL CG-3, such as {\sc addcohort}, {\sc remcohort}, {\sc move}, {\sc jump}, which on the whole make the formalism uncomparable to CG-1 and CG-2. Same holds for the dependency features.}
%However, it is clear that the morphological disambiguation rules of VISL CG-3 stay within the regular boundaries. As for the dependency rules
%\todo{check if Bick 2015 or VISLCG3 manual mention something???}



\paragraph{Expressivity of a grammar}


The expressivity of a single rule is well defined. However, the expressivity of the whole grammar is harder to define. 
%
%A CG has much more lightweight task than a generative grammar: the initial tokens, or symbols, come already with a list of alternatives.
%Returning to the question of central embedding,
%Assume we have a rule to disambiguate the word form \emph{who} as a relative pronoun instead of interrogative in the context ``N who V''.%---this contextual test is regular without question. 
%Now, this rule will perform the correct disambiguation for the pronoun, even if the relative clause is centrally embedded. 
%If all the other rules are successful, then the grammar has chosen a correct analysis for a sentence, 
%which exhibits non-regular phenomenon. 
%Still, all of the rules have only regular contextual tests.
%none of the rules needs to address anything but a regular phenomenon.
%The aforementioned task is easy partly because the input already contains so much information. The CG rules do not need to choose from arbitrary number of tags; it is safe to assume that \emph{who} will only be analysed as an interrogative or a relative pronoun. Furthermore, it does not need to generate only the correct embeddings: as defined in Section~\ref{reductionist-vs.licencing}, CG is aimed to provide analysis even for incorrect input.
%That said, CG is still a grammar formalism. For a moment, we ignore the practical needs of natural language analysis, and ask the question: ``Can it express languages beyond regular?'' 
%
%
To our knowledge, there is no work that tries to answer the latter question for CG; however, the question has been explored for other reductionist formalisms.

Anssi Yli-Jyrä \cite{yli-jyra2005phd} investigates the computational complexity of the related formalism FSIG; 
similarly to CG, it can be seen as a description of a relation rather than a
language \cite{koskenniemi97}.
%Similarly to CG, FSIG describes a relation from an input to an output; however, in order to have a superficial comparison to generative grammars, Yli-Jyrä defines the \emph{language} of a FSIG 
%as the set of the sentences which satisfy all the constraints in the grammar.
%
% Yli-Jyrä defines the set of \emph{grammatical model strings} 
% as those sentences which satisfy all the constraints in the grammar. In addition, Yli-Jyrä \cite{yli-jyra2005phd} defines the \emph{language} of each FSIG as the set of grammatical model strings; ignoring the input language of the relation, FSIGs become superficially comparable to generative grammars.
%
The main results are twofold: on the level of output, Yli-Jyrä shows that an individual FSIG grammar can approximate 
context-free grammars \cite{yli-jyra2003fsig_approx_cfg} 
and dependency grammars \cite{yli-jyra2004fsig_dependency}. 
On the level of input, \cite{yli-jyra2003describing} shows that all the contextual tests for the English FSIG grammar \cite{voutilainen1994designing} can be converted into \emph{star-free regular expressions}; a more restricted set within the class of regular languages.

% ``In this dissertation, the language of each FSIG will be uniquely the set of grammatical model strings, which makes FSIG superficially comparable to GES grammars.
 



\def\wwf{~~~~\t{"<w>"}}
\def\alm{\t{"a"}~~~~~~}
\def\blm{\t{"b"}~~~~~~}

\begin{figure}[ht]
\centering

%\begin{tabular}{cl @{\hspace{2cm}} rl}
\begin{tabular}{cl |  rl}
\multicolumn{2}{c|}{\textbf{Input}} & \multicolumn{2}{c}{\textbf{Output}} \\ \hline

\wwf  &        &  \wwf &        \\
         & \alm  &          & \alm  \\
         & \blm  &          &        \\
\wwf  &        &  \wwf &        \\
         & \alm  &          & \alm  \\
         & \blm  &          &        \\
\wwf  &        &  \wwf &        \\
         & \alm  &          & \blm  \\
         & \blm  &          &        \\
\wwf  &        &  \wwf &        \\
         & \alm  &          & \blm  \\
         & \blm  &          &        \\ 
\end{tabular}

\caption{Recognising the context-free language $a^nb^n$ in CG.}
\label{fig:anbn}
\end{figure}


As for another related formalism, Pasi Tapanainen \cite{tapanainen1999phd} demonstrates the expressive power of Functional Dependency Grammar \cite{tapanainen97fdg}.
Instead of applying rules only to natural language texts, Tapanainen shows 
two examples of FDG grammars that can express structures beyond regular: 
the first grammar can successfully create a dependency structure for the context-free language of balanced parentheses, %bracketing language, which consists of only well-nested sequences of \t{[} and \t{]}, 
and the second grammar correctly parses the context-sensitive language $a^nb^nc^n$.

We can sketch some requirements for a similar experiment for CGs.
In order to do this, we introduce the concept of \emph{maximally ambiguous sentence}: a finite sequence of cohorts, where every cohort starts off with every possible reading in the alphabet $\Sigma$. 
% In addition, we may need some helper labels $\Sigma'$, such as $\{even, odd, start, end\}$, which may not be present in the final analysis.
Figure~\ref{fig:anbn} shows the input and output of a hypothetical CG, which takes cohorts
of $\Sigma = \{a, b\}$ as an input, and removes $b$ from the first $n$ words, and $a$ from the last $n$ words. 
For strings of uneven length, the grammar would either leave it ambiguous, which would be interpreted as a sign of rejection, or use the {\sc remcohort} operation of VISL CG-3, and make the whole string empty.

Writing such grammars, or proving they cannot be written, is out of scope of this thesis.
However, we wish to bring to attention the concept maximally ambiguous sentence---which 
will be made use of in Chapter~\ref{chapterCGana}---as a way of generating sequences with CG,
and better answering the question about expressivity.
The hypothesis is that if the input language is always $\Sigma{}*$, then we may be able to reason more easily about the grammar itself. 
If we can achieve more precise definition what CG grammars can express, 
it may open new possibilities: for instance, automatically deriving CGs from other grammar formalisms. 



% We add the additional requirement that the output of the grammar may only contain readings in $\Sigma$.

%\todo{Should I explain here how the $a^nb^n$ grammar works? Or just say ``if we can write such grammar in a variant of CG, we can argue that this variant of CG is at least context-free''?}

%We illustrate the power of CG rules with the context-free language $a^nb^n$.
%Assume that the input sentence is a finite sequence, where every cohort is maximally ambiguous, ie. it contains all possible readings in $\Sigma = \{a, b\}$, and some helper labels (such as $\{even, odd, start, end\}$). Then, if we can write a set of rules that ``disambiguates'' the input as shown in Figure~\ref{fig:anbn}, we can argue that CG can express context-free languages. The same argument goes for languages beyond context-free. \todo{Kokke and Listenmaa (unpublished)} show such grammars in VISL CG-3 for the languages $a^nb^n$ and $a^nb^nc^n$



% \paragraph{Effect of ordering and execution schemes}

% We can show that the $a^nb^n$ grammar, at least in the form described above,
% requires sequential execution: in the beginning, the status of the positions is 
% clear at the edges, and the certainty spreads towards the middle, with the help of
% the careful context.
% In a parallel execution, we have the initial context during the whole round.
% If we can do parallel with multiple runs, it should work. But not just with one round.

% How about strict and heuristic?

% \paragraph{Note on termination} 
% Running the grammar multiple times is desirable: especially the rules
% with a careful context may need other rules to run first and
% disambiguate some items.
% With the most basic operations, SELECT and REMOVE, we can ensure that
% applying a grammar to a text will finish. The text might not be fully
% disambiguated, but if there is a point where running the rules again
% doesn't make a change, then the execution is stopped.

% If we allow the addition of arbitrary tags to readings, or arbitrary
% readings to cohorts, we cannot guarantee that running the grammar will
% terminate. It is possible to add the same reading over and over again,
% in which case every time we run the grammar, the argument is changed.
% The syntactic tags used by MAP rules have an additional property,
% that one reading can have at most one \texttt{@tag}, and if a reading has one
% already, it is substituted with the old one. One can end up in a loop
% substituting \texttt{@A} with \texttt{@B}.
% However, specific implementations can freely add heuristics to stop
% this kind of behaviour from happening.


