\newcommand{\quality}[1]{${\tt Quality_{#1}}$}
\newcommand{\kind}[1]{${\tt Kind_{#1}}$}
\newcommand{\very}[1]{${\tt Very_{#1}}$}
\newcommand{\comment}{${\tt Comment}$}
% \newcommand{\modFun}[2]{${\tt Mod_{#1\times#2}}$}
% \newcommand{\predFun}[3]{${\tt Pred_{#1\times#2\times#3}}$}
% \newcommand{\itemSpa}[2]{${\tt Item_{#1\times#2}}$}
\newcommand{\modFun}[2]{${\tt Mod_{#1,#2}}$}
\newcommand{\predFun}[3]{${\tt Pred_{#1,#2,#3}}$}
\newcommand{\itemSpa}[2]{${\tt Item_{#1,#2}}$}
\newcommand{\itemEng}[1]{${\tt Item_{#1}}$}
\def\t#1{\texttt{#1}}

\section{Constraint Grammar}
\label{sec:cg-intro}

Constraint Grammar (\onlycg{}) is a formalism for 
disambiguating morphologically analyzed text. 
It was first introduced by Fred Karlsson  
\cite{karlsson1990cgp,karlsson1995constraint}, and has been used for
many tasks in computational linguistics, such as part-of-speech
tagging, surface syntax and machine translation \cite{bick2011}.
\onlycg{}-based taggers are reported to achieve F-scores of over 99 \% for
morphological disambiguation, and around 95-97 \% for syntactic analysis
\cite{bick2000palavras,bick2003hybridCG_PSG,bick2006spanish}. 
\onlycg{} disambiguates morphologically analyzed input by using constraint
rules which can select or remove a potential analysis (called
\emph{reading}) for a target word, depending on the context words
around it.  Together these rules disambiguate the whole text.


In the example below, we show an initially ambiguous sentence ``the bear
sleeps''. 
It contains three word forms, such as \t{"<bear>"}, each followed by its \emph{readings}.
A reading contains one lemma, such as \t{"bear"}, and a list of morphological tags, such as \t{noun sg}.
%Additional lemmas within one word form, such as clitic pronouns, are represented as \emph{subreadings}; each indented one more tab.
A word form together with its readings is called a \emph{cohort}. A cohort is ambiguous, if it contains more than one reading.

\begin{figure}[h]
\centering
\ttfamily
\begin{tabular}{p{0.6cm} l  p{0.6cm} l}
"<the>"  &                & "<sleeps>"        \\
    & "the" det def       &     & "sleep" noun pl \\
"<bear>" &                &     & "sleep" verb pres p3 sg \\
    & "bear" noun sg      & "<.>"                   \\
    & "bear" verb pres    &     & "." sent          \\
    & "bear" verb inf \\
\end{tabular}
\label{fig:theBearSleeps}
\caption{Ambiguous sentence {\em the bear sleeps.}}
\end{figure}


\noindent We can disambiguate this sentence with two rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item \texttt{REMOVE verb IF (-1 det)}
  `Remove verb after determiner'
\item  \texttt{REMOVE noun IF (-1 noun)}
  `Remove noun after noun'
\end{enumerate}

\noindent Rule 1 matches the word \emph{bear}: it is tagged as verb and is
preceded by a determiner. The rule removes both verb readings from
\emph{bear}, leaving it with an unambiguous analysis \texttt{noun sg}.
Rule 2 is applied to the word \emph{sleeps}, and it removes the noun
reading. The finished analysis is shown below:

\begin{itemize}
\item[] 
\begin{verbatim}
"<the>"
        "the" det def
"<bear>"
        "bear" noun sg
"<sleeps>"
        "sleep" verb pres p3 sg
\end{verbatim}
\end{itemize}

It is also possible to add syntactic tags and dependency structure within \onlycg{} \cite{vislcg3,bick2015}.
However, for the remainder of this introduction, we will illustrate the examples with the 
most basic operations, that is, disambiguating morphological tags.
The syntactic disambiguation and dependency features are not fundamentally
different from morphological disambiguation: the rules describe an \emph{operation}
performed on a \emph{target}, conditional on a \emph{context}.

\subsection{Related work}

\onlycg{} is one in the family of shallow and reductionist grammar
formalisms. Disambiguation using constraint rules dates back to 1960s
and 1970s---the closest system to modern \onlycg{} was Taggit
\cite{taggit}, which was used to tag the Brown Corpus.  Karlsson
\cite{karlsson1995constraint} lists various approaches to the
disambiguation problem, including manual intervention, statistical
optimisation, unification and Lambek calculus.  For disambiguation
rules based on local constraints, Karlsson mentions
\cite{herz1991local,hindle1989disamrules}.

\onlycg{} itself was introduced in 1990. Around the same time, a related
formalism was proposed: finite-state parsing and disambiguation system
using constraint rules \cite{koskenniemi90}, which was later named
Finite-State Intersection Grammar (\fsig{}) \cite{piitulainen1995}.
Like \onlycg{}, a \fsig{} grammar contains a set of rules which remove
impossible readings based on contextual tests, in a parallel manner: a
sentence must satisfy all individual rules in a given \fsig{}.  Due to
these similarities, the name Parallel Constraint Grammar was also
suggested \cite{koskenniemi97}.
% However, \fsig{} rules usually target syntactic phenomena, and the rules allow for more expressive contextual tests than \onlycg{}. 
% In this thesis, we use the name \fsig{} to refer to the framework that is aimed at producing a full syntactic analysis with the more expressive rules, 
% and PCG to describe any \onlycg{}-variant which happens to be parallel.
% Thus, we will call implementations such as \cite{voutilainen1994designing} an \emph{\fsig{} grammar} and a program that parses it an \emph{\fsig{} parser}.
% Conversely, the \onlycg{} parser in \cite{listenmaa_claessen2015} 
% and the ``\onlycg{}-like'' system in \cite{lager98} are instances of PCG.
%We return to the comparison with \fsig{} in Sections~\ref{sec:ordering}~and~\ref{sec:expressivity}.

Brill tagging \cite{brill1995} is based on transformation rules: the
starting point of an analysis is just one tag, the most common one,
and subsequent rule applications transform one tag into another, based
on contextual tests. Like \onlycg{}, Brill tagging is known to be efficient
and accurate. The contextual tests are very similar: Lager
\cite{lager01transformation} has automatically learned both Brill rules and \onlycg{} rules, using the same system.

Similar ideas to \onlycg{} have been also explored in other frameworks, such as finite-state automata \cite{gross1997local,grana2003fst},
%, such as Local grammars \cite{gross1997local} and finite-state contextual rules \cite{grana2003fst},
logic programming \cite{oflazer97votingconstraints,lager98}, 
constraint satisfaction \cite{padro1996csp}, 
and dependency syntax \cite{tapanainen97fdg}. 
 In addition, there are a number of reimplementations of \onlycg{} using finite-state methods \cite{yli-jyra2011cg_engine,hulden2011cg_engine,peltonen2011}. 

\subsection{Properties of Constraint Grammar}\label{sec:properties}

Karlsson \cite{karlsson1995constraint} lists 24 design principles and describes
related work at the time of writing.
Here we summarize a set of main features, and relate \onlycg{} to the developments in grammar formalism since the initial description.

\onlycg{} is a \emph{reductionistic} system: the analysis starts from a list of alternatives,
and removes those which are impossible or improbable.
\onlycg{} is designed primarily for analysis, not generation; its task is 
to give correct analyses to the words in given sentences,
not to describe a language as a collection of ``all and only the grammatical sentences''.

The syntax is decidedly \emph{shallow}: the rules do not aim to
describe all aspects of an abstract phenomenon such as noun phrase; 
rather, each rule describes bits and pieces with concrete conditions.
The rules are self-contained and mutually independent---this makes it 
easy to add exceptions, and exceptions to exceptions, without 
changing the more general rules.
% The rules are self-contained and independent.
% On the one hand, this provides no guarantee that a grammar is internally consistent.
% On the other hand, these features provide flexibility that is hard to mimic by a deeper formalism.
% As we have seen in the previous sections, rules can target individual words
% or other properties that are not generalisable to a whole word class,
% such as verbs that express cognitive processes.
% Introducing a subset of verbs, even if they are used only in one rule,
% is very cheap and does not create a complicated taxonomy of different verb types.

% Most importantly, the independence of rules makes \onlycg{} highly robust.
% If one of the words is unknown or misspelt, a generative grammar would fail to produce any analysis. 
% \onlycg{} would, at worst, just leave that part ambiguous, and do as good a job it can elsewhere in the sentence.


There are different takes on how \emph{deterministic} the rules are.
The current state-of-the-art \onlycg{} parser VISL CG-3 executes the rules strictly 
based on the order of appearance, but there are other implementations which 
apply their own heuristics, or remove the ordering completely, 
applying the rules in parallel. 
%---furthermore, there are several implementations of \onlycg{} with different kind of application orders,
%such as ``apply in the order introduced in the grammar file'' or ``apply in order of longer matching context''.
A particular rule set may be written with one application order in mind, but another party may 
run the grammar with another implementation---if there are any conflicting rule pairs, then the behaviour of the grammar is different.
For that reason, we decided to apply software testing and verification methods to \onlycg{} grammars.


\subsection{Ordering and execution of the rules}\label{sec:ordering}

The previous properties of Constraint Grammar formalism and rules were
specified by Karlsson \cite{karlsson1995constraint}, and retained in further
implementations.  However, in the two decades following the initial
specification, several independent implementations have experimented
with different ordering schemes. In the present section, we describe
the different parameters of ordering and execution: \emph{strict
  vs. heuristic}, and \emph{sequential vs. parallel}.  Throughout the
section, we will apply the rules to the following ambiguous passage,
``{\em What question is that}'':

\begin{tabular}{p{0.6cm} l  p{0.6cm} l p{0.6cm} l p{0.6cm} l}
\t{"<what>"}     &                      &  \t{"<question>"}        &  & \t{"<is>"}         & & \t{"<that>"} \\
                 & \t{"what" det}       & &      \t{"question" noun}  &  &    \t{"be" verb}  & &    \t{"that" det}  \\
                 & \t{"what" pron}      & &      \t{"question" verb}  &  &                   & &    \t{"that" rel} \\
\end{tabular}


\def\satcgMax{SAT-CG\textsubscript{Max}}
\def\satcgOrd{SAT-CG\textsubscript{Ord}}
\def\noncg#1{{\em \color{gray} #1}}

\begin{table}[h!]
\centering

  \begin{tabular}{r | p{2.5cm} | p{3.5 cm} | p{3.5cm}}
           & \textbf{Strict} & \textbf{Heuristic} & \textbf{Unordered} \\ \hline
\textbf{Sequential}
           & CG-1 \cite{karlsson1990cgp}   
                             & CG-2 \cite{tapanainen1996} % (Tapanainen1996, p. 34
                                                  & --           \\ 
           & VISL CG-3  \cite{vislcg3}      
                             & Weighted CG-3 \cite{pirinen2015} & \\ 
           & Peltonen 2011 \cite{peltonen2011}  &  &\\ %Peltonen2011, p. 80
           & Yli-Jyrä 2011 \cite{yli-jyra2011cg_engine}  & & \\ 
           & Huldén 2011 \cite{hulden2011cg_engine} & & \\ \hline
\textbf{Parallel}
           & \satcgOrd       & \satcgMax          & \noncg{Lager 1998 \cite{lager98}} \\ 
           &                 & \noncg{FSIG (Voutilainen) \cite{voutilainen1994designing}} 
                                                  & \noncg{FSIG (Koskenniemi) \cite{koskenniemi90}} \\
           &                 & \noncg{Voting constraints \cite{oflazer97votingconstraints}}  \\


  \end{tabular}
  \caption{Combinations of rule ordering and execution strategy.}
  \label{table:nelikentta}
\end{table}

\paragraph{Strict vs. heuristic}

aka. ``In which order are the rules applied to a single cohort?''

An implementation with \emph{strict order} applies each rule in the
order in which they appear in the file. If a grammar contains the
rules \t{REMOVE v IF (-1 det)} and \t{REMOVE n IF (-1 pron)} in the
given order, the rule that removes the verb reading in \emph{question}
will be applied first. After it has finished, there are no verb
readings available anymore for the second rule to fire.
					
How do we know which rule is the right one? There can be many rules
that fit the context, but we choose the one that just happens to
appear first in the rule file.  A common design pattern is to place
rules with a long list of conditions first; only if they do not apply,
then try a rule with less conditions. For a similar effect, a
\emph{careful mode} may be used: ``remove verb after
\emph{unambiguous} determiner'' would not fire on the first round, but
it would wait for other rules to clarify the status of \emph{what}.
					
An alternative solution to a strict order is to use a \emph{heuristic
  order}: when disambiguating a particular word, find the rule that
has the longest and most detailed match. Now, assume that there is a
rule with a longer context, such as \t{SELECT n IF (-1 det) (1 v)},
even if this rule appears last in the file, it would be preferred to
the shorter rules, because it is a more exact match.  There are also
methods that use explicit weights to favour certain rules, such as
\cite{pirinen2015} for CG, and
\cite{voutilainen1994designing,oflazer97votingconstraints,silfverberg2009conflict}
for related formalisms.
					
Both methods have their strengths and weaknesses. A strict order is
more predictable, but it also means that the grammar writers need to
pay more thought to rule interaction. A heuristic order frees the
grammar writer from finding an optimal order, but it can give
unexpected results, which are harder to debug.  As for major CG
implementations, CG-1 \cite{karlsson1990cgp} and VISL CG-3
\cite{vislcg3} follow the strict scheme, whereas CG-2
\cite{tapanainen1996} is heuristic\footnote{Note that CG-2 is
  heuristic \emph{within sections}: the rules in a given section are
  executed heuristically, but all of them will be applied before any
  rule in a later section.}.





\paragraph{Sequential vs. parallel}

aka. ``When does the rule application take effect?''
%aka. ``Does the context change immediately after each rule is applied?''

The input sentence can be processed in sequential or parallel manner.
In \emph{sequential execution}, the rules are applied to one word at a
time, starting from the beginning of the sentence. The sentence is
updated after each application. If the word \emph{what} gets
successfully disambiguated as a pronoun, then the word \emph{question}
will not match the rule \t{REMOVE v IF (-1 det)}.


In contrast, a \emph{parallel execution} strategy disambiguates all
the words at the same time, using their initial, ambiguous context.
To give a minimal example, assume we have a single rule,
\texttt{REMOVE verb IF (-1 verb)}, and the three words \emph{can can
  can}, shown below.  In parallel execution, both \emph{can$_2$} and
\emph{can$_3$} lose their verb tag; in sequential only \emph{can$_2$}.

\begin{figure}[h]
\begin{tabular}{p{0.5cm} l   p{0.5cm} l    p{0.5cm} l}
\t{"<can$_1$>"}      &  & \t{"<can$_2$>"}  & & \t{"<can$_3$>"} & \\
   & \t{"can" noun}  &  & \t{"can" noun} & & \t{"can" noun}   \\
   & \t{"can" verb}  &  & \t{"can" verb} & & \t{"can" verb} 

\end{tabular}
\end{figure}

The question of parallel execution becomes more complicated if there
are multiple rules that apply for the same context.  Both \t{REMOVE v
  IF (-1 det)} and \t{REMOVE n IF (-1 pron)} would match
\emph{question}, because the original input from the morphological
analyser contains both determiner and pronoun as the preceding word.
The result depends on various details: shall all the rules also act in
parallel?  If we allow rules to be ordered, then the result will not
be any different from the same grammar in sequential execution; that
is, the later rule (later by any metric) will not apply.  The only
difference is the reason why not: ``context does not match'' in
sequential, and ``do not remove the last reading'' in parallel.

However, usually parallel execution is combined with \emph{unordered}
rules.  In order to express the result of these two rules in an
unordered scheme, we need a concept that has not been discussed so
far, namely, disjunction: ``the allowed combinations are {\em
  det}+{\em n} or {\em pron}+{\em v}''.  If we wanted to keep the
purely list-based ontology of CG, but combine it with a parallel and
unordered execution, then the result would have to inconclusive and
keep both readings; both cannot be removed because that would leave
{\em question} without any readings.  The difference between the
list-based and disjunction-based ontologies, corresponding to CG and
FSIG respectively, is explained with further detail in
\cite{lager_nivre01}.

Table~\ref{table:nelikentta} shows different systems of the constraint
rule family, with rule order (strict vs. heuristic) on one axis, and
execution strategy (sequential vs. parallel) on other.  Traditional CG
implementations are shown in a normal font; other, related systems in
cursive font and lighter colour.  \satcgMax and \satcgOrd refer to the
systems by the author; they are presented in
\cite{listenmaa_claessen2015} and in Chapter~\ref{chapterCGSAT} of
this thesis.


\section{Grammatical Framework}
\label{sec:gf-intro}

Grammatical Framework (\gf{}) \cite{ranta2011gfbook} is a framework for
building multilingual grammar applications. Its main components are a
functional programming language for writing grammars and a resource
library that contains the linguistic details of many natural
languages.  A \gf{} program consists of an \emph{abstract syntax} (a set
of functions and their categories) and a set of one or more
\emph{concrete syntaxes} which describe how the abstract functions and
categories are linearised (turned into surface strings) in each
respective concrete language. The resulting grammar describes a
mapping between concrete language strings and their corresponding
abstract trees (structures of function names).  This mapping is
bidirectional---strings can be \emph{parsed} to trees, and trees
\emph{linearised} to strings.  As an abstract syntax can have multiple
corresponding concrete syntaxes, the respective languages can be
automatically \emph{translated} from one to the other by first parsing
a string into a tree and then linearising the obtained tree into a new
string.

Another main component of \gf{} is the Resource Grammar Library (RGL)
\cite{ranta2009rgl}, which, as of July 2018, contains the linguistic
details of 40 natural languages. The library has had over 50
contributors, and it consists of 1900 program modules and 3 million
lines of code. As the name suggests, the RGL modules are not meant to
be used for translation as such; instead, they are used as libraries
to build smaller, domain-specific application grammars.

\subsection{Related work}

\gf{} comes from the theoretical background of type theory and logical
frameworks. The prime example of a system which combines logic and
linguistic syntax is Montague grammar \cite{montague}; in fact, \gf{}
can be seen as a general framework for Montague-style grammars.

The notion of abstract and concrete syntax appeared in both computer
science, specifically compiler construction \cite{mccarthy62}, and
linguistics, introduced by Haskell Curry \cite{curry1961} as
\emph{tectogrammatical} (abstract) and \emph{phenogrammatical}
(concrete) structure.

\gf{} is analogous to a multi-source multi-target compiler---a program
in any programming language can be parsed into the common abstract
syntax, and linearised into any of the other programming languages
that the compiler supports.  In the domain of linguistics, Ranta
\cite{ranta2011gfbook} mentions a few grammar formalisms that also
build upon abstract and concrete syntax, such as
\cite{deGroote2001acg,pollard2004hog,muskens2001lambda}. However, none
of these systems have focused on multilinguality.
The inspiration for a large resource grammar used as a library to
build smaller applications comes from  CLE (Core Language Engine)
\cite{CLE,cle2}.


\begin{figure}[h]
\centering

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abstract }\DataTypeTok{Foods} \FunctionTok{=} \NormalTok{\{}
  \NormalTok{flags startcat }\FunctionTok{=} \DataTypeTok{Comment} \NormalTok{;}
  \NormalTok{cat}
    \DataTypeTok{Comment} \NormalTok{; }\DataTypeTok{Item} \NormalTok{; }\DataTypeTok{Kind} \NormalTok{; }\DataTypeTok{Quality} \NormalTok{;}
  \NormalTok{fun}
    \DataTypeTok{Pred}\OtherTok{ :} \DataTypeTok{Item} \OtherTok{->} \DataTypeTok{Quality} \OtherTok{->} \DataTypeTok{Comment} \NormalTok{;                 }\CommentTok{-- this wine is good}
    \DataTypeTok{This}\NormalTok{, }\DataTypeTok{That}\NormalTok{, }\DataTypeTok{These}\NormalTok{, }\DataTypeTok{Those}\OtherTok{ :} \DataTypeTok{Kind} \OtherTok{->} \DataTypeTok{Item} \NormalTok{;           }\CommentTok{-- this wine}
    \DataTypeTok{Mod}\OtherTok{ :} \DataTypeTok{Quality} \OtherTok{->} \DataTypeTok{Kind} \OtherTok{->} \DataTypeTok{Kind} \NormalTok{;                     }\CommentTok{-- Italian wine}
    \DataTypeTok{Wine}\NormalTok{, }\DataTypeTok{Cheese}\NormalTok{, }\DataTypeTok{Fish}\NormalTok{, }\DataTypeTok{Pizza}\OtherTok{ :} \DataTypeTok{Kind} \NormalTok{;}
    \DataTypeTok{Warm}\NormalTok{, }\DataTypeTok{Good}\NormalTok{, }\DataTypeTok{Italian}\NormalTok{, }\DataTypeTok{Vegan}\OtherTok{ :} \DataTypeTok{Quality} \NormalTok{;}
\end{Highlighting}
\end{Shaded}
  \caption{Abstract syntax of a \gf{} grammar about food}
\label{fig:foods}
\end{figure}

\subsection{Abstract syntax}


Abstract syntax describes the constructions in our grammar without
giving a concrete implementation. Figure~\ref{fig:foods}
shows the abstract syntax of a small example grammar in \gf{}, slightly
modified from \cite{ranta2011gfbook}, and Figure~\ref{fig:spanish}
shows a corresponding Spanish concrete syntax. We refer to this
grammar throughout the thesis. 

Section~\t{cat} introduces the categories of the grammar: \t{Comment},
\t{Item},  \t{Quality}, and \t{Kind}.  \t{Comment} is the \emph{start
  category} of the grammar: this means that only comments are complete
constructions in the language, everything else is an intermediate
stage. \t{Quality} describes properties of foods, such as
\t{Warm} and \t{Good}. %, and it can be used both for modification and predication.
\t{Kind} is a basic type for foodstuffs such as \t{Wine} and
\t{Pizza}: we know what it is made of, but everything else is
unspecified. In contrast, an \t{Item} is \emph{quantified}: we know if
it is singular or plural (e.g. `one pizza' vs. `two pizzas'), definite or
indefinite (`the pizza' vs. `a pizza'), and other such things (`your
pizza' vs. `my pizza'). 

Section~\t{fun} introduces functions: they are either lexical
items without arguments, or syntactic functions which manipulate their
arguments and build new terms. Of the syntactic functions, \t{Pred}
constructs an \t{Comment} from an \t{Item} and a \t{Quality},
building trees such as \t{Pred~(This~Pizza)~Good} `this pizza is
good'. 
\t{Mod}~adds an \t{Quality} to a \t{Kind}, e.g. \t{Mod~Italian~Pizza}
`Italian pizza'. The functions  \t{This, That, These} 
and \t{Those} quantify a \t{Kind} into an \t{Item}, for instance,
\t{That~(Mod~Italian~Pizza)} `that Italian pizza'.

\begin{figure}[h]
\centering
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{concrete }\DataTypeTok{FoodsSpa} \KeywordTok{of} \DataTypeTok{Foods} \FunctionTok{=} \NormalTok{\{}
  \NormalTok{lincat}
    \DataTypeTok{Comment} \FunctionTok{=} \DataTypeTok{Str} \NormalTok{;}
    \DataTypeTok{Item} \FunctionTok{=} \NormalTok{\{ s }\OtherTok{:} \DataTypeTok{Str} \NormalTok{; n }\OtherTok{:} \DataTypeTok{Number} \NormalTok{; g }\OtherTok{:} \DataTypeTok{Gender} \NormalTok{\} ;}
    \DataTypeTok{Kind} \FunctionTok{=} \NormalTok{\{ s }\OtherTok{:} \DataTypeTok{Number} \OtherTok{=>} \DataTypeTok{Str} \NormalTok{; g }\OtherTok{:} \DataTypeTok{Gender} \NormalTok{\} ;}
    \DataTypeTok{Quality} \FunctionTok{=} \NormalTok{\{ s }\OtherTok{:} \DataTypeTok{Number} \OtherTok{=>} \DataTypeTok{Gender} \OtherTok{=>} \DataTypeTok{Str} \NormalTok{; p }\OtherTok{:} \DataTypeTok{Position} \NormalTok{\} ;}
  \NormalTok{lin}
    \DataTypeTok{Pred} \NormalTok{np ap }\FunctionTok{=} \NormalTok{np}\FunctionTok{.}\NormalTok{s }\FunctionTok{++} \NormalTok{copula }\FunctionTok{!} \NormalTok{np}\FunctionTok{.}\NormalTok{n }\FunctionTok{++} \NormalTok{ap.s }\FunctionTok{!} \NormalTok{np}\FunctionTok{.}\NormalTok{n }\FunctionTok{!} \NormalTok{np}\FunctionTok{.}\NormalTok{g ;}
    \DataTypeTok{This}\NormalTok{ cn} \FunctionTok{=} \NormalTok{mkItem }\DataTypeTok{Sg} \StringTok{"este"} \StringTok{"esta"} \NormalTok{cn ;}
    \DataTypeTok{These}\NormalTok{ cn} \FunctionTok{=} \NormalTok{mkItem }\DataTypeTok{Pl} \StringTok{"estos"} \StringTok{"estas"} \NormalTok{cn ;}
    \CommentTok{-- That, Those defined similarly}
    \DataTypeTok{Mod} \NormalTok{ap cn }\FunctionTok{=} \NormalTok{\{ s }\FunctionTok{=} \NormalTok{\textbackslash\textbackslash{}n }\OtherTok{=>} \NormalTok{preOrPost ap}\FunctionTok{.}\NormalTok{p (ap}\FunctionTok{.}\NormalTok{s }\FunctionTok{!} \NormalTok{n }\FunctionTok{!} \NormalTok{cn}\FunctionTok{.}\NormalTok{g) (cn}\FunctionTok{.}\NormalTok{s }\FunctionTok{!} \NormalTok{n) ;}
                  \NormalTok{g }\FunctionTok{=} \NormalTok{cn}\FunctionTok{.}\NormalTok{g \} ;}
    \CommentTok{--Wine, Cheese, \dots, Italian, Vegan defined as lexical items}
  \NormalTok{param}
    \DataTypeTok{Number} \FunctionTok{=} \DataTypeTok{Sg} \FunctionTok{|} \DataTypeTok{Pl} \NormalTok{;}
    \DataTypeTok{Gender} \FunctionTok{=} \DataTypeTok{Masc} \FunctionTok{|} \DataTypeTok{Fem} \NormalTok{;}
    \DataTypeTok{Position} \FunctionTok{=} \DataTypeTok{Pre} \FunctionTok{|} \DataTypeTok{Post} \NormalTok{;}
  \NormalTok{oper}
\NormalTok{    mkItem} \NormalTok{num mascDet femDet cn} \FunctionTok{=}
     \KeywordTok{let} \NormalTok{det }\FunctionTok{=} \KeywordTok{case} \NormalTok{cn}\FunctionTok{.}\NormalTok{g }\KeywordTok{of} \NormalTok{\{ }\DataTypeTok{Masc} \OtherTok{=>} \NormalTok{mascDet ; }\DataTypeTok{Fem} \OtherTok{=>} \NormalTok{femDet \} ;}
      \KeywordTok{in} \NormalTok{\{ s }\FunctionTok{=} \NormalTok{det }\FunctionTok{++} \NormalTok{cn}\FunctionTok{.}\NormalTok{s }\FunctionTok{!} \NormalTok{num} \NormalTok{;} \NormalTok{n }\FunctionTok{=} \NormalTok{num} \NormalTok{; g }\FunctionTok{=} \NormalTok{cn}\FunctionTok{.}\NormalTok{g \} ;}
    \NormalTok{copula }\FunctionTok{=} \NormalTok{table \{ }\DataTypeTok{Sg} \OtherTok{=>} \StringTok{"es"} \NormalTok{; }\DataTypeTok{Pl} \OtherTok{=>} \StringTok{"son"} \NormalTok{\} ;}
    \NormalTok{preOrPost p x y }\FunctionTok{=} \KeywordTok{case} \NormalTok{p }\KeywordTok{of} \NormalTok{\{ }\DataTypeTok{Pre} \OtherTok{=>} \NormalTok{x }\FunctionTok{++} \NormalTok{y ; }\DataTypeTok{Post} \OtherTok{=>} \NormalTok{y }\FunctionTok{++} \NormalTok{x \} ;}
\end{Highlighting}
\end{Shaded}
  \caption{Spanish concrete syntax of a \gf{} grammar about food}
\label{fig:spanish}
\end{figure}

\subsection{Concrete syntax}
\label{concrete_spanish_foods}
Concrete syntax is an implementation of the abstract syntax.
The section~\t{lincat} corresponds to \t{cat} in the abstract syntax:
for every abstract category introduced in \t{cat}, we give a concrete
implementation in \t{lincat}.


Figure~\ref{fig:spanish} shows the Spanish concrete
syntax, in which \t{Comment} is a string, and the rest of the
categories are more complex records. For instance, \t{Kind} has a
field \t{s} which is a table from number to string (\textsc{sg} $\Rightarrow$
\emph{pizza}, \textsc{pl} $\Rightarrow$ \emph{pizzas}), and another
field \t{g}, which contains its gender (feminine for \t{Pizza}). We
say that \t{Kind} has \emph{inherent} gender, and \emph{variable} number. 

The section~\t{lin} contains the concrete implementation of the
functions, introduced in \t{fun}. Here we handle
language-specific details such as agreement: when \t{Pred (This Pizza)
  Good} is linearized in Spanish, `esta pizza es buena', the copula must be singular
(\emph{es} instead of plural \emph{son}), and the adjective must be in singular
feminine (\emph{buena} instead of masculine \emph{bueno} or plural
\emph{buenas}), matching the gender of \t{Pizza} and the number of \t{This}. 
If we write an English concrete syntax, then only the number of the copula is
relevant: this pizza/wine \emph{is} good, these pizzas/wines \emph{are} good.


\subsection{PMCFG}
\label{sec:PMCFG}

\gf{} grammars are compiled into parallel multiple context-free
grammars (\pmcfg), introduced by Seki et al. \cite{seki91pmcfg}. The
connection between \gf{} and \pmcfg{} was established by Peter
Ljunglöf \cite{ljunglof2004}, and further developed by Krasimir
Angelov \cite{angelov2010phd}.  Here we explain three key features,
which will be used for the test suite generation; the concrete
implementation details can be found in Angelov
\cite{angelov2010phd}. We include also a more formal definition of
\pmcfg{} in Appendix~\ref{pmcfg-formally}.

\paragraph{Concrete categories}

For each category in the original grammar, the \gf{} compiler
introduces a new \emph{concrete category} in the \pmcfg{} for each combination of
inherent parameters.  
These concrete categories can be linearized to strings or vectors of
strings. The start category (\t{Comment} in the Foods grammar) is in
general a single string, but intermediate categories may have to keep
several options open. 

Consider the categories \t{Item}, \t{Kind} and \t{Quality} in the
Spanish concrete syntax. Firstly, \t{Item} has inherent number
and gender, so it compiles into four concrete categories:
\itemSpa{sg}{masc}, \itemSpa{sg}{fem}, \itemSpa{pl}{masc} and
\itemSpa{pl}{fem}, each of them containing one string. Secondly,
\t{Kind} has an inherent gender and variable number, so it compiles into
two concrete categories: \kind{masc} and \kind{fem}, each of them a
vector of two strings (singular and plural). Finally, \t{Quality} needs to
agree in number and gender with its head, but it has its position as
an inherent feature.  Thus \t{Quality} compiles into two concrete
categories: \quality{pre} and \quality{post}, each of them a vector of
four strings.
% ({\stackanchor{\tt \small pl}{\tt \small sg}}
%  $\times$ {\stackanchor{\tt \small masc}{\tt \small fem}}).

\paragraph{Concrete functions}
Just like categories, each syntactic function from the original
grammar turns into multiple syntactic functions into the
\pmcfg{}---one for each combination of parameters of its arguments.

\begin{itemize}
\setlength\itemsep{0em}
\item \modFun{pre}{fem~~} \t{:} \quality{pre~} $\rightarrow$ \kind{fem~} $\rightarrow$ \kind{fem}
\item  \modFun{post}{fem~} \t{:} \quality{post} $\rightarrow$ \kind{fem~} $\rightarrow$ \kind{fem}
\item  \modFun{pre}{masc~~}\t{:} \quality{pre~} $\rightarrow$ \kind{masc} $\rightarrow$ \kind{masc}
\item \modFun{post}{masc} \t{:} \quality{post} $\rightarrow$ \kind{masc} $\rightarrow$ \kind{masc}
\end{itemize}


\paragraph{Coercions}
\label{sec:Coercions}
As we have seen, \t{Quality} in Spanish compiles into \quality{pre} and
\quality{post}. However, the difference of position is meaningful only when the
adjective is modifying the noun: ``la \emph{buena} pizza'' vs. ``la pizza
\emph{vegana}''. But when we use an adjective in a predicative position, both
classes of adjectives behave the same: ``la pizza es \emph{buena}''
and ``la pizza es \emph{vegana}''. As an optimization strategy, the
grammar creates a {\it coercion}: both \quality{pre} and \quality{post}
may be treated as \quality{*} when the distinction doesn't matter. 
Furthermore, the function \t{Pred : Item $\rightarrow$ Quality $\rightarrow$ S} uses
the coerced category \quality{*} as its second argument, and thus
expands only into 4 variants, despite there being 8 combinations of
\t{Item}$\times$\t{Quality}.

\begin{itemize}
\setlength\itemsep{0em}
\item \predFun{sg}{fem}{*~} \t{:} \itemSpa{sg}{fem~} $\rightarrow$ \quality{*} $\rightarrow$ \comment
\item  \predFun{pl}{fem}{*~} \t{:} \itemSpa{pl}{fem~} $\rightarrow$ \quality{*} $\rightarrow$ \comment
\item  \predFun{sg}{masc}{*} \t{:} \itemSpa{sg}{masc} $\rightarrow$ \quality{*} $\rightarrow$ \comment
\item \predFun{pl}{masc}{*} \t{:} \itemSpa{pl}{masc} $\rightarrow$ \quality{*} $\rightarrow$ \comment
\end{itemize}