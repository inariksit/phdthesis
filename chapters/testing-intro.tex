\section{Software testing and verification}
\label{sec:testing-intro} 

\todo{This whole section is just a draft.}

\begin{itemize}
\item All software has bugs.
\item Grammars are software.
\end{itemize}

The intelligent reader may complete the syllogism.

We can approach the elimination of bugs in two ways: find them
by devising clever tests, or build safeguards into the program that
make it more difficult for bugs to occur in the first place. The
latter approach is more of a concern for developers of new programming
languages---the language can impose more checks and constraints on the
programmer, and thus make it harder to write buggy code. However, we
approach the problem from particular, existing grammar formalisms that
already have millions of lines of code written in them. Thus, we want
to develop methods that help finding bugs in existing software.

% Simplified model of programming languages: if it's easy to write
% programs in a language, it's easy to write buggy programs. There are
% languages that impose more checks and constraints, and thus make it
% harder to write buggy code, but the overall effect is that it's harder
% to write \emph{any} code. Some argue that the combination ``easy to
% write correct code, hard to write buggy code'' exists, but the actual
% language to achieve this varies from person to person.\footnote{There
%   is more of a consensus of the existence of programming languages
%   that are hard to write correct but easy to write buggy.} 

% However interesting these points are from programming language
% research point of view, they are way beside the point for this
% thesis. We approach the problem from particular, existing grammar
% formalisms that already have millions of lines of code written in
% them. Thus, we want to develop methods that help finding bugs in
% existing software. 

\subsection{Purpose of testing}

Ammann and Offutt \todo{cite \url{http://assets.cambridge.org/97811071/72012/excerpt/9781107172012_excerpt.pdf}}
define different levels of enlightment for software engineers.

\begin{quote}
Level 0:  There is no difference between testing and debugging. \\
Level 1: The purpose of testing is to show correctness. \\
Level 2: The purpose of testing is to show that the software does not work. \\
Level 3: The purpose of testing is not to prove anything specific, but to reduce the risk of using the software. \\
Level 4: Testing is a mental discipline that helps all IT professionals develop higher-quality software.
\end{quote}



\subsection{Testing}

More quotes from the Intro to Software testing book:

\begin{quote}
Testing: Evaluating software by observing its execution \\
Test Failure: Execution of a test that results in a software failure \\
Debugging: The process of finding a fault given a failure \\
\\
Not all inputs will “trigger” a fault into causing a failure
\\
\\
Reachability :The location or locations in the program that contain the fault must be reached \\
Infection :The state of the program must be incorrect \\
Propagation :The infected state must cause some output or  final state of the program to be incorrect \\
Reveal :The tester must observe part of the incorrect portion of the program state \\
\end{quote}

http://twiki.di.uminho.pt/twiki/pub/Research/CROSS/Publications/techReport-ATGsurvey.pdf

You have a \emph{system}, you give it \emph{input} and 
\emph{expected output}.
Then you run the system with your input, and compare \emph{actual
  output} to expected output.

Some examples different types of tests:

\paragraph{Unit testing}

Unit tests are particular, concrete test cases: assume we want to test the
addition function (+), we could write some facts we know, such as
``1+2 should be 3''. In the context of grammars and natural language,
we could assert translation equivalence between a pair of strings,
e.g. ``\emph{la casa grande} $\Leftrightarrow$ \emph{the big house}'',
or between a tree and its linearisation, e.g. ``\t{DetCN the\_Det
  (AdjCN  big\_A house\_N)} $\Leftrightarrow$ \emph{the big house}''.
Whenever a program is changed or updated, the collection of unit tests
are run again, to make sure that the change has not broken something
that previously worked.

\paragraph{Property-based testing}

The weakness of unit testing is that it only tests cases that the
developer has thought of adding. Another approach is to use
property-based testing: the developer defines abstract properties that
should hold for all test cases, and uses random generation to supply
values. If we want to test the (+) function again, we could write a
property that says ``for all integers $x$ and $y$, $x+y$ should be
equal to $y+x$''.  A grammar-related property could be, for instance,
``a linearisation of a tree must contain strings from all of its
subtrees''.  We formulate these properties, and generate a large
number of data---pairs of integers in the first case, syntax trees in
the second---and assert that all of them follow the property.

% Of course, the single property of commutativity is not enough to
% define that (+) works as intended: if that were the only test, it
% would happily pass any function with similar property,
% e.g. multiplication, or some function $f(x, y) = 0$ for any $x$ and
% $y$. Likewise, some components in a syntax tree may not contribute
% with a string, but only a parameter to choose the right inflection
% form from another subtree.
%  To remedy this, we would need to test other
% properties of addition, or mix in some unit tests with specific
% values.

\paragraph{Symbolic evaluation}

There are lots of things that can happen during the run of a
program. What happens if we get a 3 from the user as an input? Does it
crash if it's 29th of February and we get a 3 from the user?  Symbolic 
evaluation simulates the run of a program with different inputs, using 
a constraint solver to find out where certain combinations lead into.

\paragraph{Deriving test cases}

Unit tests, as well as properties, can be written by a human or derived
automatically from some representation of the program. This can range
from informal descriptions, such as specifications or user stories, to
individual statements in the source code. Alternatively, tests can be
generated from an abstract model of the program, such as a UML
diagram.

In the context of grammar testing, the specification is the whole
language of the grammar---hardly a formal and agreed-upon
document. Assuming no access to the computational grammar itself
(sometimes called \emph{black-box testing}), we could treat
traditional grammar books as an inspiration for properties and unit
tests. For example, we can test a feature such as ``pronouns must be
in an accusative case after a preposition'' by generating example
sentences where pronouns occur after a preposition, and reading
through them to see if the property holds.

If we have access to the grammar while designing tests
(\emph{white-box testing}), we can take advantage of the
structure---to start with, only test those features that are
implemented in the grammar. 

% We can name
% three levels of abstraction where to generate tests. \emph{Black-box
%   testing} relies on external descriptions of the software, without
% access to the source code. A tester would not care how the program
% produces the intended output, just that its output is correct. In
% contrast, \emph{white-box testing} has access to the source code, and
% can exploit concrete details of the program, such as individual
% conditions and statements. Finally, \emph{model-based testing} derives
% tests from some model of a program, such as a UML diagram.



\paragraph{Coverage criteria}

When have we tested enough? We need structured ways of covering the input
space.

We are testing a grammar, which is necessarily an imperfect
approximation of the language it is based on. Thus, even if the
grammar generates an infinite amount of sentences, we still have only
a finite set of constructions and grammatical functions to cover.





\input{chapters/sat-intro}
